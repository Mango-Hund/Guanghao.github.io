<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>吴恩达机器学习（三） | Liu's blog</title><meta name="description" content="机器学习经典入门"><meta name="keywords" content="Python,machine-learning"><meta name="author" content="Guanghao"><meta name="copyright" content="Guanghao"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://lguanghao.com/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="吴恩达机器学习（三）"><meta property="og:url" content="http://lguanghao.com/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"><meta property="og:site_name" content="Liu's blog"><meta property="og:description" content="机器学习经典入门"><meta property="og:image" content="http://lguanghao.com/img/ai.jpg"><meta property="article:published_time" content="2020-07-02T10:45:38.000Z"><meta property="article:modified_time" content="2020-07-25T05:25:04.873Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="Unity3D入门" href="http://lguanghao.com/2020/07/09/Unity3D%E5%85%A5%E9%97%A8/"><link rel="next" title="吴恩达机器学习（二）" href="http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-07-25 13:25:04'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">20</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#无监督学习"><span class="toc-number">1.</span> <span class="toc-text">无监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#无监督学习的概念"><span class="toc-number">1.1.</span> <span class="toc-text">无监督学习的概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K-Means算法"><span class="toc-number">1.2.</span> <span class="toc-text">K-Means算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#优化目标"><span class="toc-number">1.3.</span> <span class="toc-text">优化目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#随机初始化"><span class="toc-number">1.4.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#选取聚类数量"><span class="toc-number">1.5.</span> <span class="toc-text">选取聚类数量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#降维"><span class="toc-number">2.</span> <span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#目标1-数据压缩"><span class="toc-number">2.1.</span> <span class="toc-text">目标1 数据压缩</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#目标2-可视化"><span class="toc-number">2.2.</span> <span class="toc-text">目标2 可视化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#主成分分析问题规划"><span class="toc-number">2.3.</span> <span class="toc-text">主成分分析问题规划</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#主成分数量的选择"><span class="toc-number">2.4.</span> <span class="toc-text">主成分数量的选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩重现（解压）"><span class="toc-number">2.5.</span> <span class="toc-text">压缩重现（解压）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#应用PCA的建议"><span class="toc-number">2.6.</span> <span class="toc-text">应用PCA的建议</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#异常检测"><span class="toc-number">3.</span> <span class="toc-text">异常检测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#高斯分布（-正态分布）"><span class="toc-number">3.1.</span> <span class="toc-text">高斯分布（&#x2F;正态分布）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#算法"><span class="toc-number">3.2.</span> <span class="toc-text">算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#开发和评估异常检测系统"><span class="toc-number">3.3.</span> <span class="toc-text">开发和评估异常检测系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#异常检测-VS-监督学习"><span class="toc-number">3.4.</span> <span class="toc-text">异常检测 VS 监督学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#怎样选择特征"><span class="toc-number">3.5.</span> <span class="toc-text">怎样选择特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多变量高斯分布"><span class="toc-number">3.6.</span> <span class="toc-text">多变量高斯分布</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#推荐系统"><span class="toc-number">4.</span> <span class="toc-text">推荐系统</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#问题规划"><span class="toc-number">4.1.</span> <span class="toc-text">问题规划</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基于内容的推荐算法"><span class="toc-number">4.2.</span> <span class="toc-text">基于内容的推荐算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#协同过滤"><span class="toc-number">4.3.</span> <span class="toc-text">协同过滤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#矢量化：低秩矩阵"><span class="toc-number">4.4.</span> <span class="toc-text">矢量化：低秩矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实施细节：均值归一化"><span class="toc-number">4.5.</span> <span class="toc-text">实施细节：均值归一化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#大规模机器学习"><span class="toc-number">5.</span> <span class="toc-text">大规模机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#随机梯度下降"><span class="toc-number">5.1.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mini-Batch-梯度下降"><span class="toc-number">5.2.</span> <span class="toc-text">Mini-Batch 梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#随机梯度下降收敛"><span class="toc-number">5.3.</span> <span class="toc-text">随机梯度下降收敛</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#在线学习"><span class="toc-number">5.4.</span> <span class="toc-text">在线学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#减少映射与数据并行"><span class="toc-number">5.5.</span> <span class="toc-text">减少映射与数据并行</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#应用举例-照片OCR"><span class="toc-number">6.</span> <span class="toc-text">应用举例 照片OCR</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#问题描述"><span class="toc-number">6.1.</span> <span class="toc-text">问题描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#滑动窗口分类器"><span class="toc-number">6.2.</span> <span class="toc-text">滑动窗口分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#获取大量数据和人工数据"><span class="toc-number">6.3.</span> <span class="toc-text">获取大量数据和人工数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#天花板分析：下一步的流程线"><span class="toc-number">6.4.</span> <span class="toc-text">天花板分析：下一步的流程线</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/mountains.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Liu's blog</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">吴恩达机器学习（三）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-07-02 18:45:38"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-07-02</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 13:25:04"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/study/">study</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">5.4k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 20 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><h4 id="无监督学习的概念"><a href="#无监督学习的概念" class="headerlink" title="无监督学习的概念"></a>无监督学习的概念</h4><p>在无监督学习中，我们的数据并不带有任何标签。聚类算法有很多的实际应用，例如市场分割、社交网络分析，组织计算集群，天文数据分析。</p>
<h4 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/51.png" alt="图51">  </p>
<ol>
<li>执行k均值算法，第一步是随机生成两点，这两点称为聚类中心。</li>
</ol>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/52.png" alt="图52"><br>K均值算法是一种迭代算法，它会做两件事，第一个是簇分配，第二个是移动聚类中心。在K均值算法中，每次内循环的第一步，是要进行簇分配，也就是说，我要遍历每个样本，也就是图上的每个绿点，然后根据每一个点，是与红色聚类中心更近，还是与蓝色聚类中心更近，来将每个数据点分配给两个聚类中心之一。具体来说，就是遍历你的数据集，然后将每个点染成红色或蓝色，这取决于某个点是更接近于红色聚类中心还是更接近蓝色聚类中心，现在已经分好了图中点。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/53.png" alt="图53"><br>这就是簇分配的步骤。<br>K均值的第二步就是移动聚类中心，我们要做的是将两个聚类中心，也就是红色和蓝色的叉，将其移动到同色的点的均值处，因为我们要做的是，找出所有红色的点，然后计算它们的均值，也就是所有的红色点的平均位置，然后把红色的聚类中心移动到这里，蓝色聚类中心也一样，找出所有蓝点并计算它们的均值，然后把蓝色聚类中心移动到那里。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/54.png" alt="图54"><br>如果不断运行K均值算法的迭代，可以得到<br><img src= "/img/loading.gif" data-src="/img/machine-learning/55.png" alt="图55"><br>K均值算法接受两个输入，一个是参数K，它表示你想从数据中聚类出的簇的个数，另一个输出就是一系列无标签的，只用x来表示的数据集。<br>Input：  </p>
<ul>
<li>K(number of clusters)</li>
<li>Training set{$x^{(1)},x^{(2)},..,x^{(m)}$}</li>
<li>$x^{i}\in\mathbb{R}^n$(drop $x_0=1$ convention)<br>K均值算法：<br>随机初始化K个聚类中心 $\mu_1,\mu_2,..,\mu_k\in\mathbb{R}^n$  <blockquote>
<p>Repeat {<br>for $i=1$ to $m$<br>  $c^{(i)}$:=index(from 1 to K) of cluster centroid closest to $x^{(i)}$<br>for $k=1$ to K<br>  $\mu_k$:=average (mean) of points assigned to cluster k<br>}  </p>
</blockquote>
</li>
</ul>
<ol>
<li>$c^{(i)}$来表示第1个到第k个，最接近$x^{(i)}$的聚类中心，这就是簇分配步骤，将每个样本，根据它离哪一个聚类中心更近一些，将其染成对应的颜色。所以$c^{(i)}$是一个在1到k之间的数，它表明这个点是更接近红色的叉还是蓝色的叉。  </li>
<li>$\mu_k$代表这个簇中所有点的均值</li>
</ol>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>K均值的优化目标函数</p>
<blockquote>
<p>$c^{(i)}$ = index of cluster(1,2..,K)to which wxample $x^{(i)}$ is currently assigned<br>$\mu_k$ = cluster centroidk ($\mu_k\in\mathbb{R}$)<br>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>
</blockquote>
<p>优化目标函数(失真函数)可以表示为：<br>$$J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_k)=\frac{1}{m}\parallel x^{(i)}-\mu_{c^{(i)}}\parallel^2$$<br>$$\min\limits_{c^{(1)},…,c^{(m)},\mu_1,…,\mu_k}J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_k)$$</p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>当运行K均值算法时，你应该把聚类中心数值K设置为比训练样本数量m小的值。如果运行一个K均值算法，它的聚类中心数等于或大于样本数会很奇怪。<br>随机挑选K个训练样本，然后设定$\mu_1$到$\mu_k$,让它们等于这K个样本。<br>假设K等于2,为了初始化聚类中心，随机挑选两个样本，要初始化聚类中心，只需要将这两个样本作为聚类中心。<br>K均值算法可能会落在局部最优，如果随机初始化不好，则会得到较差的局部最优。我们能做的就是初始化K均值算法很多次，并运行K均值算法很多次，以此来保证我们最终能得到一个足够好的结果，具体做法为：  </p>
<blockquote>
<p>For i = 1 to 100{<br>    Randomly initialize K-means.<br>    Run K-means. Get $c^{(1)},…,c^{(m)},\mu_1,…,\mu_k$.<br>    Compute cost function (distortion)<br>     $J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_k)$<br>}<br>Pick clustering that gave lowest cost $J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_k)$  </p>
</blockquote>
<h4 id="选取聚类数量"><a href="#选取聚类数量" class="headerlink" title="选取聚类数量"></a>选取聚类数量</h4><p>选择聚类数量，肘部法则，通过改变K值来计算目标优化函数的值并画图。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/56.png" alt="图56"><br>有时，您正在运行K-means以获得群集以用于稍后的/下游用途。根据指标评估K均值，以了解该指标在以后的目的中的表现。也就是根据目的去选择K的数量。  </p>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><h4 id="目标1-数据压缩"><a href="#目标1-数据压缩" class="headerlink" title="目标1 数据压缩"></a>目标1 数据压缩</h4><p>有些数据的特征是可以削减的，例如厘米和英尺这两个特征。<br>2D to 1D 将数据投影到直线<br>3D to 2D 将数据投影到平面  </p>
<h4 id="目标2-可视化"><a href="#目标2-可视化" class="headerlink" title="目标2 可视化"></a>目标2 可视化</h4><p>多维数据不能可视化，需要降维呈现。降维后特征所表述的意义可以通过图像来理解。</p>
<h4 id="主成分分析问题规划"><a href="#主成分分析问题规划" class="headerlink" title="主成分分析问题规划"></a>主成分分析问题规划</h4><p>降维操作最常用的算法是一个叫做主成分分析（PCA）的算法。找一个低维平面，然后将高维数据都投影在该平面上。<br>在应用PCA之前，常规的做法是先进性均值归一化和特征规范化，使得特征均值为0，并且其数值在可比较范围之内。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/57.png" alt="图57"><br>从2维减少到1维：找到要在其上投影数据的方向（向量$u^{(1)}\in\mathbb{R}^n$，以最大程度地减少投影误差。<br>PCA并不是线性回归<br><img src= "/img/loading.gif" data-src="/img/machine-learning/58.png" alt="图58"><br>PCA试图找到一个低维的平面，来对数据进行投影，以便最小化投影误差的平方，以及最小化每个点与投影后的对应点之间距离的平方值。  </p>
<p>数据预处理<br>训练集：$x^{(1)},x^{(2)},…,x^{(m)}$<br>预处理（特征缩放/均值标准化）：<br>$$\mu_j=\frac{1}{m}\sum^m_{i=1}x_j^{(i)}$$<br>用$x_j-\mu_j$去代替$x_j^{(i)}$.<br>如果不同的特征规模不同的话，（例如$x_1=size\quad of \quad house,x_2=number\quad of \quad bedrooms$）,缩放特征量以具有可比较的值范围。</p>
<p>主成分分析（PCA）算法<br>将数据从n维降到k维<br>计算“协方差”:<br>$$\Sigma=\frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T$$</p>
<blockquote>
<p>From <code>[U,S,V]=svd(Sigma)</code>, we get:<br>$U=\begin{bmatrix} |&amp;|&amp;…&amp;| \ u^{(1)}&amp;u^{(2)}&amp;…&amp;u^{(n)} \ |&amp;|&amp;…&amp;| \end{bmatrix}\in\mathbb{R}^{n\times n}$<br>take the first K Columns of the U matrix</p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/59.png" alt="图59">  </p>
<h4 id="主成分数量的选择"><a href="#主成分数量的选择" class="headerlink" title="主成分数量的选择"></a>主成分数量的选择</h4><p>选择K（主成分数量）<br>平均投影误差平方：$\frac{1}{m}\sum\limits^m_{i=1}\parallel x^{(i)}-x_{approx}^{(i)}\parallel^2$<br>数据的总变化：$\frac{1}{m}\sum\limits^m_{i=1}\parallel x^{(i)}\parallel^2$<br>通常，将k选择为最小值，以便<br>$$\frac{\frac{1}{m}\sum\limits_{i=1}^m\parallel x^{(i)}-x_{approx}^{(i)}\parallel^2} {\frac{1}{m}\sum\limits^m_{i=1}\parallel x^{(i)}\parallel^2}\leq0.01 \qquad (1%)$$<br>“保留99％的方差信息”<br><img src= "/img/loading.gif" data-src="/img/machine-learning/60.png" alt="图60">  </p>
<blockquote>
<p>Pick smallest value of k for which<br>$\frac{\sum\limits^k_{i=1}S_{ii}}{\sum\limits^m_{i=1}S_{ii}}\geq0.99$<br>(99% of variance retained)  </p>
</blockquote>
<h4 id="压缩重现（解压）"><a href="#压缩重现（解压）" class="headerlink" title="压缩重现（解压）"></a>压缩重现（解压）</h4><p>$z^{(i)}=U^T_{reduce}x^{(i)}$<br>$x^{(i)}=U_{reduce}z^{(i)}$</p>
<h4 id="应用PCA的建议"><a href="#应用PCA的建议" class="headerlink" title="应用PCA的建议"></a>应用PCA的建议</h4><p>利用PCA对监督学习算法进行加速<br>假设有一个监督学习的例子<br>$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$<br>如果$x^{(i)}\in\mathbb{R}^{10000}$,对于这种高维的特征向量，运行学习算法时将变得非常慢。使用PCA可以减少数据的维度，从而使得算法运行更加高效。  </p>
<ol>
<li>首先可以检查已经被标记的训练集，并抽取输入，把y暂时放在一边,所以我们就得到了一个无标签的训练集。<br>Unlabeled dataset：$x^{(1)},x^{(2)},..,x^{(m)}\in\mathbb{R}^{10000}$  </li>
<li>接着我们使用PCA得到原始数据的低维表示$z^{(1)},z^{(2)},..,z^{(m)}\in\mathbb{R}^{1000}$  </li>
<li>这就给了我们一个新的训练集$(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),…,(z^{(m)},y^{(m)})$  </li>
<li>我们可以将新的训练集输入假设函数，进行计算，但不能应用在交叉验证集或者在测试集上。  </li>
</ol>
<p>！！PCA不可以去修缮过拟合的问题</p>
<h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h3><p>假如你是一个飞机引擎生产商，在生产飞机引擎时，你需要进行质量控制测试，而作为这个测试一部分，你测量了飞机引擎的一些特征变量。例如：<br>Aircraft engine features:<br>$x_1 = heat\quad generated$<br>$x_2 = vibration\quad intensity$<br>…<br>Dataset:{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>加入有一天有了一个新引擎，有一个特征变量集$x_{test}$。所谓的异常检测问题就是，我们希望知道这个新的飞机引擎是否有某种异常。<br>定义：<br>数据集：{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>这个数据集里是正常的，那新的$x_{test}$是否正常呢？<br>欺诈识别:<br>$x^{(i)}$=用户活动的特征<br>Model $p(x)$ from data<br>通过 $p(x)&lt;\varepsilon$ 检查哪些非正常用户    </p>
<h4 id="高斯分布（-正态分布）"><a href="#高斯分布（-正态分布）" class="headerlink" title="高斯分布（/正态分布）"></a>高斯分布（/正态分布）</h4><p>对于$x\in\mathbb{R}$. 如果x服从均值为$\mu$,方差为$\sigma^2$的正态分布。 可写作：<br>$x\sim N(\mu,\sigma^2)$<br>$$p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/61.png" alt="图61"><br>参数估计<br>Dataset：{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>假设这些样本来自一个高斯分布的总体，假设我猜测每一个样本$x^{(i)}$服从高斯分布，但是我们不知道$\mu$和$\sigma^2$是多少。参数估计就是从数据集中估计出未知的参数。<br>$$\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$$<br>$$\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2$$</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>概率密度估算<br>Training set:{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>Each example is $x\in\mathbb{R}^m$<br>$x_1\sim N(\mu_1,\sigma_1^2)$<br>$x_2\sim N(\mu_2,\sigma_3^2)$<br>$x_3\sim N(\mu_3,\sigma_3^2)$<br>…<br>$x_m\sim N(\mu_m,\sigma_m^2)$<br>$p(x)=p(x_1;\mu_1,\sigma^2_1)p(x_2;\mu_2,\sigma^2_2)…p(x_m;\mu_m,\sigma^2_m)=\prod\limits_{j=1}^m p(x_j;\mu_j,\sigma^2_j)$  </p>
<ol>
<li>选择你认为服从正态分布的特征量$x_i$</li>
<li>去估算出参数</li>
<li>给出新的样板x，计算p(x)<br>$$p(x)=\prod\limits_{j=1}^m p(x_j;\mu_j,\sigma^2_j)=\prod\limits_{j=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$<br>Anomaly if p(x) &lt; $\varepsilon$  </li>
</ol>
<h4 id="开发和评估异常检测系统"><a href="#开发和评估异常检测系统" class="headerlink" title="开发和评估异常检测系统"></a>开发和评估异常检测系统</h4><p>假设我们有一个数据集：$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$(假设是正常的样本/没有异常的样本)<br>交叉验证集:$(x_cv^{(1)},y_cv^{(1)}),…,(x_cv^{(m)},y_cv^{(m)})$<br>测试集：$(x_test^{(1)},y_test^{(1)}),…,(x_test^{(m)},y_test^{(m)})$<br>如果有10000个好的飞机引擎，有20个异常的飞机引擎<br>训练集：6000个好引擎<br>交叉验证集：2000个好引擎（y=0），10个异常引擎(y=1)<br>测试集：2000个好引擎(y=0)，10个异常引擎(y=1)<br>算法应用：</p>
<blockquote>
<p>Fit model p(x) on training set{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>On a cross validation/test example x, predict<br>$y=\begin{cases}1 &amp; if&amp;p(x)&lt;\varepsilon&amp;(anomaly)\ 0 &amp; if&amp;p(x)\geq\varepsilon&amp;(nomaly)\end{cases}$<br>Possible evalution metric:<br>1 True positive, false positive, false negative, true negative<br>2 Precision/Recall<br>3 $F_1$-score<br>Can also use cross validation set to choose parameter $\varepsilon$  </p>
</blockquote>
<h4 id="异常检测-VS-监督学习"><a href="#异常检测-VS-监督学习" class="headerlink" title="异常检测 VS 监督学习"></a>异常检测 VS 监督学习</h4><ul>
<li>异常检测<br>如果一个遇到一个正常的样本数量很少，而异常的样本有很多的情况。 任何算法都很难从正例中学习异常现象。 未来的异常可能看起来与我们到目前为止所看到的任何异常示例都不相似。  </li>
<li>监督学习<br>大量的正面和负面的例子。足够的正例可以使算法理解正例，未来的正例可能与训练集中的正例相似。</li>
</ul>
<h4 id="怎样选择特征"><a href="#怎样选择特征" class="headerlink" title="怎样选择特征"></a>怎样选择特征</h4><p>如果你画出数据的直方图， 并且发现图形看起来与高斯分布相差甚远，那么就有必要进行一些不同的转换，通过这些方法让数据更接近高斯分布。</p>
<p>在检测异常的算法中，我们希望p(x)在正常样本时比较大，在异常样本的情况下比较小。一种常见的情况是，如果p(x)是可比较的，当正常样本和异常时p(x)的值都比较大。看看一些异常的样本会不会启发自己创造新的特征。  </p>
<p>选择在发生异常情况时可能具有异常大或小的值的特征。</p>
<h4 id="多变量高斯分布"><a href="#多变量高斯分布" class="headerlink" title="多变量高斯分布"></a>多变量高斯分布</h4><p>$x\in\mathbb{R}^n.$不要分开建立模型$p(x_1),p(x_2),…,etc.$ 要建立统一的模型。<br>参数 $\mu\in\mathbb{R}^n,\Sigma\in\mathbb{R}^{n\times n}$(协方差矩阵)<br>$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}\mid\Sigma\mid^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)T\Sigma^{-1}(x-\mu))$$<br>参数估计：<br>给出训练集：{$x^{(1)},x^{(2)},…,x^{(m)}$}<br>$\mu=\frac{1}{m}\sum\limits_{i=1}^mx^{(i)}$<br>$\Sigma=\frac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$  </p>
<ol>
<li>$p(x)$ 参数估计<br>$\mu=\frac{1}{m}\sum\limits_{i=1}^mx^{(i)}$<br>$\Sigma=\frac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$  </li>
<li>给出一个新样本，计算<br>$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}\mid\Sigma\mid^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)T\Sigma^{-1}(x-\mu))$$<br>标记为异常如果$p(x)&lt;\varepsilon$<br>原始的高斯模型是多元高斯模型的一种特殊情况。  </li>
</ol>
<ul>
<li>原始模型：手动创建特征以捕获异常，其中$x_1,x_2$采用不寻常的值组合。<br>Computaionally cheaper(alternatively,scales better to large n)<br>OK even if m (training set size) is small</li>
<li>多元模型：自动捕获要素之间的关联<br>Computationally more expensive<br>Must have m&gt;n, or else $\Sigma$ is non-invertible</li>
</ul>
<h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><h4 id="问题规划"><a href="#问题规划" class="headerlink" title="问题规划"></a>问题规划</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/62.png" alt="图62"><br>我们根据此想开发一个推荐系统<br>一个可以自动填补这些缺失值的算法，这样我们就可以看一下，该用户还有哪些电影没有看过，并推荐新电影给该用户。 </p>
<h4 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/63.png" alt="图63"><br>For each user j, learn a parameter $\theta^{(j)}\in\mathbb{R^{n+1}}$. Predict user j as rating movie i with $(\theta^{(j)})^Tx^{(i)}$ stars. </p>
<p>$r(i,j)$ = 1 如果用户评价了电影i，我们就将$r(i,j)$记为1.，否则记为0.<br>$y^{(i,j)}$ = 对该电影的评价，如果评价存在的话<br>$\theta^{(j)}$ = 每个用户 $x^{(i)}$ 的一个参数。<br>$x^{(i)}$ = 特定电影的一个特征向量<br>对于每一个用户和电影，我们会预测：$(\theta^{(j)})^T(x^{(i)})$<br>$m^{(j)}$ = 评价了电影的用户数量</p>
<p>为了学习参数向量$\theta^{(j)}$:<br>$$\min\limits_{\theta^{(j)}}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum^n_{k=1}(\theta_k^{(j)})^2$$<br>为了学习所有用户的$\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$:<br>$$\min\limits_{\theta^{(j)}}\sum^{n_u}<em>{j=1}\sum</em>{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum^{n_u}<em>{j=1}\sum^n</em>{k=1}(\theta_k^{(j)})^2$$<br>下面，为了实现最小化，采用梯度下降的方法：<br>$$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}\quad(for\quad k = 0)$$<br>$$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda\theta_k^{(j)})\quad(for\quad k \neq 0)$$</p>
<h4 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h4><p>如果你有所有电影评分的集合，即 $r(i,j)$、$y^{(i,j)}$,于是根据不同电影的特征，我们可以学习不同用户的参数$\theta$<br>如果你的用户愿意为你提供这些参数，你就能估计出各种电影的特征值。<br>$\theta-x-\theta-x…$不断互相推导，互相改进，最终算法会收敛到一个合理的系统推荐模型。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/64.png" alt="图64">  </p>
<p>将这两个优化函数结合为一个函数，同时最小化 $x^{(1)},x^{(2)},…,x^{(n_m)}$ 和 $\theta^{(1)},\theta^{(2)},…,\theta^{(m)}$:<br>$$J(x^{(1)},..,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum\limits_{(i,j):r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})^2+\frac{\lambda}{2}\sum^{n_m}<em>{i=1}\sum^n</em>{k=1}(x_k^{(i)})^2+\frac{\lambda}{2}\sum^{n_u}<em>{j=1}\sum^n</em>{k=1}(\theta_k^{(j)})^2$$<br>$$\min\limits_{x^{(1)},x^{(2)},…,x^{(n_m)},\theta^{(1)},\theta^{(2)},…,\theta^{(m)}}J(x^{(1)},..,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/65.png" alt="图65">   </p>
<h4 id="矢量化：低秩矩阵"><a href="#矢量化：低秩矩阵" class="headerlink" title="矢量化：低秩矩阵"></a>矢量化：低秩矩阵</h4><p>找到相关的电影：<br>对于每一个产品i，我们学习的特征向量为 $x^{(i)}\in\mathbb{R}^n$.<br>怎么样去找到与电影i相关的电影j。<br>5 个相似于电影i的电影：<br>找到5个电影，满足smllest$\parallel x^{(i)}-x^{(j)}\parallel$.  </p>
<h4 id="实施细节：均值归一化"><a href="#实施细节：均值归一化" class="headerlink" title="实施细节：均值归一化"></a>实施细节：均值归一化</h4><p>如果有个人从来不对电影进行评价，那就无法给他推荐相关的电影。均值归一化的想法可以让我们解决这个问题。和以前一样我们把所有的评分放在矩阵Y里，就是把所有的这些评分，全部整合到矩阵Y里，全是？的一列对应Eve没有给任何电影评分，要实现归一化，要做的就是计算，每个电影所得评分的均值，我们把它们存在一个叫$\mu$的向量中。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/66.png" alt="图66"><br>对于用户j，基于i预测：$(\theta^{(j)})^T(x^{(i)})+\mu_i$  </p>
<h3 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h3><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>代价函数 $cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$<br>目标优化函数<br>$$J_{train}(\theta)=\frac{1}{m}\sum^m_{i=1}cost(\theta,(x^{(i)},y^{(i)}))$$</p>
<ol>
<li>将所有m个训练样本重新随机排列</li>
<li>Repeat{<br>  for i:=1,…,m{   <pre><code>$\theta_j:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$  
(for every j=0,...,n)  </code></pre>
  }  </li>
</ol>
<p>}<br>循环1～10次就好了  </p>
<h4 id="Mini-Batch-梯度下降"><a href="#Mini-Batch-梯度下降" class="headerlink" title="Mini-Batch 梯度下降"></a>Mini-Batch 梯度下降</h4><p>批量梯度下降：在每次迭代中使用所有m个样本<br>随机梯度下降：每次迭代均使用1个样本<br>小批量梯度下降：在每次迭代中使用b个样本<br><img src= "/img/loading.gif" data-src="/img/machine-learning/67.png" alt="图67">  </p>
<h4 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h4><blockquote>
<p>Batch gradient descent:<br>Plot $J_{train}(\theta)$ as a function of the number of iterations of gradient descent.<br>$J_{train}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$<br>Stochastic gradient descent:<br>$cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$<br>During learning, compute $cost(\theta,(x^{(i)},y^{(i)}))$ before updating $\theta$ using $(x^{(i)},y^{(i)})$<br>Every 1000 iterations(say), plot $cost(\theta,(x^{(i)},y^{(i)}))$ averaged over the last 1000 examples processed by algorithm.</p>
</blockquote>
<p>比起之前的算法需要时不时地计算$J_{train}$，那样就得遍历所有的训练样本，而梯度下降法的这个步骤，只用在更新$\theta$之前计算这些cost函数，并不需要太大的计算量。我们要做就是每1000次迭代运算中，求出前1000个cost函数的平均值，然后把它画出来，通过画出来的图，我们就能检查出，随机梯度下降是否在收敛。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/68.png" alt="图68"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/69.png" alt="图69">  </p>
<h4 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h4><p>如果你有一个由不断进入网站的用户流所产生的连续的数据流，你就可以使用在线学习机制，从数据流中学习用户的偏好，然后用这些信息来优化关于网站的决策。<br>用户来到的运输服务网站，指定出发地和目的地，您提供要价的方式运输其包裹，并且用户有时选择使用您的运输服务（y = 1），有时不选择（y = 0）。<br>特征量 x 捕获用户属性，来源/目的地和要价的功能。 我们想通过学习$p(y=1|x;\theta)$以优化价格。  </p>
<h4 id="减少映射与数据并行"><a href="#减少映射与数据并行" class="headerlink" title="减少映射与数据并行"></a>减少映射与数据并行</h4><p>Map-reduce解决随机梯度下降解决不了的更大规模数据的问题。<br>批量梯度下降：$\theta_j:=\theta_j-\alpha\frac{1}{400}\sum\limits_{i=1}^400(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br>Machine 1: Use $(x^{(1)},y^{(1)}),…,(x^{(100)},y^{(100)})$<br>Machine 2: Use $(x^{(101)},y^{(101)}),…,(x^{(200)},y^{(200)})$<br>Machine 3: Use $(x^{(201)},y^{(201)}),…,(x^{(300)},y^{(300)})$<br>Machine 4: Use $(x^{(301)},y^{(301)}),…,(x^{(400)},y^{(400)})$<br>将数据分到四台机器上使得计算速度提高了四倍。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/70.png" alt="图70"><br>只要你的学习算法可以表示为对训练集的求和 </p>
<h3 id="应用举例-照片OCR"><a href="#应用举例-照片OCR" class="headerlink" title="应用举例 照片OCR"></a>应用举例 照片OCR</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>照片OCR全称为“照片光学字符识别”，其注重让计算机读出图片中的文字信息。</p>
<ol>
<li>首先，给定某张照片，它将图像扫描一遍。</li>
<li>然后找出照片中的文字信息，之后它将重点关注这些文字区域，并对区域中的文字进行识别，分割成独立字符</li>
<li>当它正确读出这些文字后，它会将这些文字内容显示并记录下来。</li>
</ol>
<p>Photo OCR pipeline（过程流水线）<br><img src= "/img/loading.gif" data-src="/img/machine-learning/71.png" alt="图71">  </p>
<h4 id="滑动窗口分类器"><a href="#滑动窗口分类器" class="headerlink" title="滑动窗口分类器"></a>滑动窗口分类器</h4><p>照片OCR的第一步，文字识别：  </p>
<ol>
<li>从数据集中收集一些正样本和负样本<br><img src= "/img/loading.gif" data-src="/img/machine-learning/72.png" alt="图72"> </li>
<li>利用固定比例的窗口滑动算法，然后将发现的白色区域放大周围范围 </li>
<li>字符分割，使用监督学习算法，提供正样本和负样本  </li>
</ol>
<h4 id="获取大量数据和人工数据"><a href="#获取大量数据和人工数据" class="headerlink" title="获取大量数据和人工数据"></a>获取大量数据和人工数据</h4><p>人工数据合成主要有两种形式，第一种实际上是自己创造数据，即从零开始创造新数据，第二种是我们已经有了小的标签训练集，然后以某种方式扩充训练集。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/73.png" alt="图73"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/74.png" alt="图74"><br>对原数据进行扩充要有代表性。  </p>
<ol>
<li>Make sure you have a low bias classifier before expending the effort.(Plot learning curves).E.g.keep increasing the number of features/number of hidden units in neural network until you have a low bias classifier.</li>
<li>“How much work would it be to get 10x as much data as we currently have?” <ul>
<li>人工合成数据</li>
<li>扩充已有数据集</li>
<li>“crowd source”</li>
</ul>
</li>
</ol>
<h4 id="天花板分析：下一步的流程线"><a href="#天花板分析：下一步的流程线" class="headerlink" title="天花板分析：下一步的流程线"></a>天花板分析：下一步的流程线</h4><p>上限分析<br><img src= "/img/loading.gif" data-src="/img/machine-learning/75.png" alt="图75"><br>哪一个模块需要花更多的时间？<br>提升哪个模块对系统性能改善最有效果？<br><img src= "/img/loading.gif" data-src="/img/machine-learning/76.png" alt="图76"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/77.png" alt="图77">      </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>First Step ， and thank you for your teaching!!!</strong><br><img src= "/img/loading.gif" data-src="/img/machine-learning/78.png" alt="图78"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/79.png" alt="图79">      </p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Guanghao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lguanghao.com/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/">http://lguanghao.com/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lguanghao.com" target="_blank">Liu's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/machine-learning/">machine-learning</a></div><div class="post_share"><div class="social-share" data-image="/img/tree.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/09/Unity3D%E5%85%A5%E9%97%A8/"><img class="prev-cover" data-src="/img/unity.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Unity3D入门</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><img class="next-cover" data-src="/img/ai.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">吴恩达机器学习（二）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/07/22/Jupyter notebook快速上手/" title="Jupyter-notebook"><img class="relatedPosts_cover" data-src="/img/jupyter.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-22</div><div class="relatedPosts_title">Jupyter-notebook</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/20/吴恩达机器学习（二）/" title="吴恩达机器学习（二）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-20</div><div class="relatedPosts_title">吴恩达机器学习（二）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/10/吴恩达机器学习（一）/" title="吴恩达机器学习（一）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-10</div><div class="relatedPosts_title">吴恩达机器学习（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/26/列表的切片问题/" title="列表切片的问题"><img class="relatedPosts_cover" data-src="/img/white-dog.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-26</div><div class="relatedPosts_title">列表切片的问题</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/20/数据分析（三）/" title="数据分析工具（三）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-20</div><div class="relatedPosts_title">数据分析工具（三）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/13/数据分析（二）/" title="数据分析工具（二）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-13</div><div class="relatedPosts_title">数据分析工具（二）</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(/img/mountains.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By Guanghao</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"right","hOffset":0,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body></html>