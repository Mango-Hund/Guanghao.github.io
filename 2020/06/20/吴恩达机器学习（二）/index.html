<!DOCTYPE html>
<html lang="zh-CN">




<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" type="image/png" href="/img/favicon.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>吴恩达机器学习（二） - Liu&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Hao-Space</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/escape_velocity.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-06-20 18:45">
      2020年6月20日 晚上
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      98
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>分类问题，例如<br>Email: 垃圾邮件/不是垃圾邮件<br>Online：信息是真的/信息是假的<br>Tumor：良性的/恶性的<br>$y={0,1}$<br>0:Negative Class负类/1:Positive Class正类<br><img src="/img/machine-learning/09.png" srcset="/img/loading.gif" alt="图9"><br>将分类器的输出$h_\theta(x)$阀值定位0.5<br>如果$h_\theta\geq0.5$,预测’y=1’<br>如果$h_\theta&lt;0.5$,预测‘y=0’ </p>
<h4 id="假设陈述"><a href="#假设陈述" class="headerlink" title="假设陈述"></a>假设陈述</h4><p>逻辑分类模型<br>我们想要$0\leq h_\theta(x) \leq 1$<br>$h_\theta(x)=g(\theta^Tx)$<br>$g(z)=\frac{1}{1+e^{-z}}$ z是实数 ，g(z)一端趋向1一端趋向0。<br>关于假设h(x)函数的解释：<br>$h_\theta(x)$= 输入x对于y=1的估计概率<br>举例：如果 $x=\begin{bmatrix}x_0 \ x_1\end{bmatrix}=\begin{bmatrix}1 \ tumorSize \end{bmatrix}$<br>$h_\theta(x)=0.7$<br>可以说明病人的肿瘤有70%的概率为恶性肿瘤。<br>当特征变量为x，变量的参数为$\theta$，可以表示‘y=1’或‘y=0’的概率为：<br>$$h_\theta(x)=P(y=1|x;\theta)$$<br>$$P(y=0|x;\theta)+P(y=1|x;\theta)=1$$<br>$$P(y=0|x;\theta)=1-P(y=1|x;\theta)$$</p>
<h4 id="决策界限（decision-boundary）"><a href="#决策界限（decision-boundary）" class="headerlink" title="决策界限（decision boundary）"></a>决策界限（decision boundary）</h4><p>分类函数：<br>$h_\theta(x)=g(\theta^Tx)=P(y=1|x;\theh<br>ta)$<br>$g(z)=\frac{1}{1+e^{-z}}$<br><img src="/img/machine-learning/10.png" srcset="/img/loading.gif" alt="图10"><br>$g(z)\geq0.5$ when $z \geq 0$。$h_\theta(x)=g(\theta^Tx)\geq0.5$ when $\theta^T\geq0$<br>假设：预测“y=1” 如果$h_\theta(x)\geq0.5$<br>预测“y=0” 如果$h_\theta(x)&lt;0.5$<br><img src="/img/machine-learning/11.png" srcset="/img/loading.gif" alt="图11"><br>假设$h_\theta(x)=g(\theta_0+\theta x_1+\theta x_2)$,并且已知$\theta^T=\begin{bmatrix}-3 &amp; 1 &amp; 1\end{bmatrix}$<br>可以预测”y=1”, 如果 $-3+x_1+x_2\geq0$，也就是$x_1+x_2\geq3$<br><img src="/img/machine-learning/12.png" srcset="/img/loading.gif" alt="图12"><br>$h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_4^2)$<br>$\theta^T=\begin{bmatrix}-1 &amp; 0 &amp; 0 &amp; 1 &amp;1\end{bmatrix}$<br>预测“y=1”，如果$-1+x_1^2+x_2^2\geq0$，可改写为$x_1^2+x_2^2\geq0$<br>一旦有了参数$\theta$，决策边界就可以确定了。</p>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>训练集：{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$}<br>m个特征 $x\in\begin{bmatrix}x_0\x_1\…\x_n\end{bmatrix}$<br>$x_0=1,y\in{0,1}$<br>$$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$$<br>我们应该怎么去确定参数$\theta$呢？<br>代价函数：<br>线性回归：<br>$$J(\theta)=\frac{1}{m}\sum^m_{i=1}\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$$<br>平均误差平方可以写为：<br>$$Cost(h_\theta(x^{(i)}),y^{(i)})=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$$<br>如果直接代入，画出代价函数（优化目标函数如左图，有很多局部最优，且是非凸函数）<br><img src="/img/machine-learning/13.png" srcset="/img/loading.gif" alt="图13"><br>逻辑回归代价函数<br>$$Cost(h_\theta(x),y)=\begin{cases}<br>-log(h_\theta(x)&amp;if&amp;y=1\<br>-log(1-h_\theta(x))&amp;if&amp;y=0<br>\end{cases}<br>$$<br><img src="/img/machine-learning/14.png" srcset="/img/loading.gif" alt="图14"><br>搞错的代价是巨大的<br><img src="/img/machine-learning/15.png" srcset="/img/loading.gif" alt="图15">    </p>
<h4 id="简化代价函数与梯度下降"><a href="#简化代价函数与梯度下降" class="headerlink" title="简化代价函数与梯度下降"></a>简化代价函数与梯度下降</h4><p>逻辑回归代价函数（优化目标函数）<br>$$J(\theta)=\frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}),y^{(i)})$$<br>$$Cost(h_\theta(x),y)=\begin{cases}<br>-log(h_\theta(x)&amp;if&amp;y=1\<br>-log(1-h_\theta(x))&amp;if&amp;y=0<br>\end{cases}<br>$$<br>Note:y=0 or 1 always<br>简化$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$<br>由此我们可以得到：<br>$$J(\theta)=-\frac{1}{m}[\sum^m_{i-1}y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$<br>为了拟合出参数$\theta$,我们应该找出让$J(\theta)$取得最小值的参数$\theta$：<br>$\min\limits_\theta J(\theta)$<br>得到了参数，当我们输入一组新的特征去预测时<br>输出：$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$<br>使用梯度下降法来找出代价函数的最小值<br>$$J(\theta)=-\frac{1}{m}[\sum^m_{i-1}y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$<br>想要得到$\min\limits_\theta J(\theta)$：<br>Repeat{<br>$$\theta_j:\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)=\theta_j-\alpha\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>(同时更新所有的$\theta_j$)<br>}<br>它与线性回归的参数求解公式一样，但是因为线性回归和逻辑回归所设定的假设函数不一样，一个是$h_\theta(x)=\theta^Tx$,一个是$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$.  </p>
<h4 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h4><p>优化算法<br>代价函数$J(\theta)$,想得到$\min\limits_\theta J(\theta)$<br>当我们输入参数$\theta$的时候，我们可以使用代码计算  </p>
<ul>
<li>$J(\theta)$</li>
<li>$\frac{\partial}{\partial\theta_j}J(\theta)\quad (for\quad j=0,1,2…,n)$<br>梯度下降：<br>Repeat {<br>$$\theta_j:\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)$$<br>}<br><img src="/img/machine-learning/16.png" srcset="/img/loading.gif" alt="图16"><br>(共轭梯度法、BFGS、L-BFGS)<br>举例：<br>$\theta=\begin{bmatrix}\theta_1 \ \theta_2\end{bmatrix}$<br>$J(\theta)=(\theta_1-5)^2+(\theta_2-5)^2$<br>$\frac{\partial}{\partial\theta_1}J(\theta)=2(\theta_1-5)$<br>$\frac{\partial}{\partial\theta_2}J(\theta)=2(\theta_2-5)$    <pre><code class="hljs lsl">funtion[jVal,gradient]
        = costFunction(theta)
    jVal = (theta(<span class="hljs-number">1</span>)<span class="hljs-number">-5</span>)^<span class="hljs-number">2</span> + ...
    (theta(<span class="hljs-number">2</span>)<span class="hljs-number">-5</span>)^<span class="hljs-number">2</span>;
    gradient = zeros(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>);
    gradient(<span class="hljs-number">1</span>) = <span class="hljs-number">2</span>*(theta(<span class="hljs-number">1</span>)<span class="hljs-number">-5</span>);
    gradient(<span class="hljs-number">2</span>) = <span class="hljs-number">2</span>*(theta(<span class="hljs-number">2</span>)<span class="hljs-number">-5</span>);
options = optimst('GradObj', 'on','MaxIter','<span class="hljs-number">100</span>');
initialTheta = zeros(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>);
[optTheta, functionVal, exitFlag] ...
    =fminunc(@costFunction,initialTheta,options)</code></pre>

</li>
</ul>
<h4 id="多元算法"><a href="#多元算法" class="headerlink" title="多元算法"></a>多元算法</h4><p>多元分类问题<br>Email 标签归类：工作、朋友、家庭和爱好<br>医学诊断：没有生病、感冒、发烧<br>天气：晴朗、多云、下雨、下雪<br><img src="/img/machine-learning/17.png" srcset="/img/loading.gif" alt="图17"><br><img src="/img/machine-learning/18.png" srcset="/img/loading.gif" alt="图18"><br>训练一个逻辑回归分类器$h_\theta^{(i)}(x)$对每一个种类进行预测。<br>新输入一个x去做分类预测，取$\max\limits_ih_\theta^{(i)}(x)$即最大值，有最好分类效果的分类器。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h4><p><img src="/img/machine-learning/19.png" srcset="/img/loading.gif" alt="图19"><br>欠拟合/正常拟合与过拟合<br>如果我们具有太多的特征，则学习的假设可能非常适合训练集，但无法归纳为新的示例（在新示例上预测价格,无法泛化一般化去预测）<br><img src="/img/machine-learning/20.png" srcset="/img/loading.gif" alt="图20"><br>有两个办法防止过拟合</p>
<ol>
<li>减少特征的数量</li>
</ol>
<ul>
<li>人为的删除某些特征变量</li>
<li>模型选择算法</li>
</ul>
<ol start="2">
<li>正则化</li>
</ol>
<ul>
<li>保留所有的特征变量，但是减少参数值的大小</li>
<li>这个方法非常有效，当我们有很多特征变量时，每个变量都可以对预测做出贡献  </li>
</ul>
<p><img src="/img/machine-learning/21.png" srcset="/img/loading.gif" alt="图21"><br>假设我们在函数中加入惩罚项，使得$\theta_3$,$\theta_4$非常小<br>$$\min\limits_\theta\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$<br>$$\min\limits_\theta\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+1000\theta_3^2+1000\theta_4^2$$<br>要想改写后的代价函数尽可能小，那么$\theta_3$,$\theta_4$的值都要尽量接近于0。也就是说我们只需要多余项的参数足够小，这样就可以避免过拟合。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让参数尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 参数 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 参数 的值接近于 0，就像我们忽略了参数值一样。如果我们做到这一点（ 参数 接近 0 ），那么我们将得到一个近似的多项式函数。<br>将参数的值减小，我们会得到一个更简单的假设模型，也更不容易出现过拟合的现象。<br>举例房屋问题：</p>
<ul>
<li>特征：$x_1,x_2,…,x_{100}$</li>
<li>参数：$\theta_0$,$\theta_1$,$\theta_2$,…,$\theta_{100}$ </li>
</ul>
<p>这里有很多特征量，并且我们一时间无法筛查出哪些是不相关的<br>$$J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h(x^{(i)})-y^{(i)})^2$$<br>改写代价函数为(正则化)：<br>$$J(\theta)=\frac{1}{2m}[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^n_{j=1}\theta_j^2]$$<br>$\lambda$称为正则化系数，它的作用是控制两个不同目标之间的取舍。<br>在正则化中，我们选择$\theta$去缩小<br>$$J(\theta)=\frac{1}{2m}[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^n_{j=1}\theta_j^2]$$<br>如果$theta$被设定为一个极大的值，例如$\theta=10^{10}$,参数的值接近于0,可能出现欠拟合的现象。  </p>
<h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p>梯度下降法：<br>Repeat<br>$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>正则化改写成：<br>$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j$$<br>$$\theta_j=:\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>使每次迭代，都使$\theta_j$乘以一个比1略小的数。<br>正规方程法：<br>$X=\begin{bmatrix}(x^{(1)})^T \…\ (x^{(m)})^T\end{bmatrix}$     $\quad y=\begin{bmatrix}y^{(1)} \…\ y^{(m)}\end{bmatrix}$<br>$$\theta=(X^TX)^{-1}X^Ty$$<br>正则化改写为：<br>$$(X^TX+\lambda\begin{bmatrix}0&amp;0&amp;0&amp;…&amp;0 \ 0&amp;1&amp;0&amp;…&amp;0 \ 0&amp;0&amp;1&amp;…&amp;0 \ 0&amp;0&amp;0&amp;…&amp;0 \ 0&amp;0&amp;0&amp;…&amp;1\end{bmatrix})^{-1}X^Ty$$  </p>
<h4 id="Logistic回归的正则化"><a href="#Logistic回归的正则化" class="headerlink" title="Logistic回归的正则化"></a>Logistic回归的正则化</h4><p>代价函数：<br>$$j(\theta)=-[\frac{1}{m}\sum_{i=1}^my^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$<br>正则化改写为：<br>$$j(\theta)=-[\frac{1}{m}\sum_{i=1}^my^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$<br>梯度下降法：<br>Repeat<br>$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>正则化改写成：<br>$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j$$<br>$$\theta_j=:\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$</p>
<h3 id="神经网络学习"><a href="#神经网络学习" class="headerlink" title="神经网络学习"></a>神经网络学习</h3><h4 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h4><p>对于一般机器学习问题，特征量都会非常多。<br><img src="/img/machine-learning/22.png" srcset="/img/loading.gif" alt="图22">  </p>
<h4 id="神经元与大脑"><a href="#神经元与大脑" class="headerlink" title="神经元与大脑"></a>神经元与大脑</h4><p>起源：试图模仿大脑的算法。 在80年代和90年代初期被广泛使用； 在90年代后期，人气下降。<br>最近的复兴：适用于许多应用的最新技术.<br><img src="/img/machine-learning/23.png" srcset="/img/loading.gif" alt="图23"><br>模拟单个神经元<br><img src="/img/machine-learning/24.png" srcset="/img/loading.gif" alt="图24"><br>模拟神经网络<br><img src="/img/machine-learning/25.png" srcset="/img/loading.gif" alt="图25"><br>$a_i^{j}=$ “激活值” of unit i in layer j<br>$\Theta^{(j)}=$ 权重控制矩阵从第 j 层到第 j+1 层的映射<br>$$a_1^{(2)}=g(\Theta^{(1)}<em>{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3)$$<br>$$a_2^{(2)}=g(\Theta^{(1)}</em>{20}x_0+\Theta^{(1)}<em>{21}x_1+\Theta^{(1)}_{22}x_2+\Theta^{(1)}_{23}x_3)$$<br>$$a_3^{(2)}=g(\Theta^{(1)}</em>{30}x_0+\Theta^{(1)}<em>{31}x_1+\Theta^{(1)}_{32}x_2+\Theta^{(1)}_{33}x_3)$$<br>$$h_\Theta(x)=a_1^{(3)}=g(\Theta^{(2)}_{10}a_0^{(2)}+\Theta^{(2)}_{11}a_1^{(2)}+\Theta^{(2)}_{12}a_2^{(2)}+\Theta^{(2)}_{13}a_3^{(2)})$$<br>如果神经网络在j层有$s_j$个单元，在 j+1 层有$s</em>{j+1}$个单元，因此$\Theta^{(j)}$将会是一个$s_{j+1}\times(s_j+1)维度的矩阵$。<br>将上述式子进行改写并向量化<br>$$a_1^{(2)}=g(z_1^{(2)})$$<br>$$a_2^{(2)}=g(z_2^{(2)})$$<br>$$a_3^{(2)}=g(z_3^{(2)})$$<br>$$<br>x=\begin{bmatrix}<br>x_0 \ x_1 \ x_2 \x_3<br>\end{bmatrix}<br>\quad z^{(2)}=\begin{bmatrix}<br> z_1^{(2)} \ z_2^{(2)} \ z_3^{(2)}<br>\end{bmatrix}<br>$$<br>$z^{(2)}=\Theta^{(1)}x=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})$<br>Add $a_0^{(2)}=1$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$h_\Theta(x)=a^{(3)}=g(z^{(3)})$<br>上述称为向前传播<br><img src="/img/machine-learning/26.png" srcset="/img/loading.gif" alt="图26">  </p>
<h4 id="例子与直觉理解"><a href="#例子与直觉理解" class="headerlink" title="例子与直觉理解"></a>例子与直觉理解</h4><p><img src="/img/machine-learning/27.png" srcset="/img/loading.gif" alt="图27"><br>Simple example:AND<br>$x_1,x_2\in{0,1}$<br>$y=x_1\quad AND\quad x_2$<br><img src="/img/machine-learning/28.png" srcset="/img/loading.gif" alt="图28"><br><img src="/img/machine-learning/29.png" srcset="/img/loading.gif" alt="图29"><br>隐藏层可以计算一些，然后下一层计算更复杂的，层数越多，计算越复杂，最后这些特征作用于输出。  </p>
<h4 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h4><p>要在神经网络中实现多类别分类，采用的方法本质上是一种一对多法的扩展。<br><img src="/img/machine-learning/30.png" srcset="/img/loading.gif" alt="图30">   </p>
<h3 id="神经网络参数的反向传播算法"><a href="#神经网络参数的反向传播算法" class="headerlink" title="神经网络参数的反向传播算法"></a>神经网络参数的反向传播算法</h3><h4 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h4><p><img src="/img/machine-learning/31.png" srcset="/img/loading.gif" alt="图31"><br>二元分类：<br>$y=0/1$<br>1个输出结果<br>多元分类：<br>$y=\mathbb{R}^K \quad E.g.\begin{bmatrix}1 \ 0 \ 0 \ 0\end{bmatrix},\begin{bmatrix}0 \ 1 \ 0 \ 0\end{bmatrix},\begin{bmatrix}0 \ 0 \ 1 \ 0\end{bmatrix},\begin{bmatrix}0 \ 0 \ 0 \ 1\end{bmatrix}$<br>K个输出。<br>逻辑回归的代价函数<br>$$J(\theta)=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta_j^2$$<br>神经网络：<br>$h_\Theta(x)\in\mathbb{R}^K,(h_\Theta(x))<em>i=i^{th}output$<br>$$j(\Theta)=-\frac{1}{m}[\sum</em>{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))<em>k+(1-y^{(i)}<em>k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}</em>{l=1}\sum^{s_l}_{i=1}\sum^{s</em>{l+1}}<em>{j=1}(\Theta^{(l)}</em>{ji})^2$$</p>
<h4 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h4><p>$$j(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))<em>k+(1-y^{(i)}<em>k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}</em>{l=1}\sum^{s_l}_{i=1}\sum^{s</em>{l+1}}<em>{j=1}(\Theta^{(l)}</em>{ji})^2$$<br>目的是$\min_\Theta J(\Theta)$<br>我们需要计算</p>
<ul>
<li>$J(\Theta)$</li>
<li>$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$  </li>
</ul>
<p><img src="/img/machine-learning/32.png" srcset="/img/loading.gif" alt="图32"><br>给一个训练样板（x，y）：<br>按照下面的顺序进行计算：<br>$a^{(1)}=x$<br>$z{(2)}=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})\quad (add\quad a_0^{(2)})$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$a^{(3)}=g(z^{(3)})\quad (add\quad a_0^{(3)})$<br>$z^{(4)}=\Theta^{(3)}a^{(3)}$<br>$a^{(4)}=h_\Theta(x)=g(z^{(4)})$<br>误差：$\delta^{(l)}<em>j=$ 第l层第j个节点的误差。<br>例如对于每一个输出（层数L=4）<br>$$\delta_j^{(4)}=a_j^{4}-y_j$$<br>$$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*g’(z^{(3)})$$<br>$$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*g’(z^{(3)})$$<br>推导见<br><a href="https://zhuanlan.zhihu.com/p/25609953" target="_blank" rel="noopener">神经网络反向传播算法详细推导</a><br>反向传播算法：<br>训练集${(x^{(1)},y^{(1)}),…,(x^{(m)},y^{(m)})}$<br>设$\Delta^{(l)}_{ij}=0(对于全部的l,i,j来说)$<br>For i=1 to m<br>Set $a^{(1)}=x^{(i)}$<br>Perform forward propagation to compute $a^{(l)}$ for l = 2,3,…,L.<br>Using $y^{(i)}$,compute $\delta^{(L)}=a^{(L)}-y^{(i)}$<br>Compute $\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$<br>$\Delta^{(l)}</em>{ij}:=\Delta^{(l)}<em>{ij}+a_j^{(l)}\delta_i^{(l+1)}$<br>我们可以得到<br>$D</em>{ij}^{(l)}:=\frac{1}{m}\Delta^{(l)}<em>{ij}+\lambda\Theta</em>{ij}^{(l)}$ if $j\neq 0$<br>$D_{ij}^{(l)}:=\frac{1}{m}\Delta^{(l)}<em>{ij}$ if $j= 0$<br>则有：$\frac{\partial}{\partial\Theta^{(l)}</em>{ij}}J(\Theta)=D_{ij}^{(l)}$</p>
<h4 id="理解反向传播算法"><a href="#理解反向传播算法" class="headerlink" title="理解反向传播算法"></a>理解反向传播算法</h4><p><img src="/img/machine-learning/33.png" srcset="/img/loading.gif" alt="图33"><br>反向传播算法做了什么工作？<br>$$j(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))<em>k+(1-y^{(i)}<em>k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}</em>{l=1}\sum^{s_l}_{i=1}\sum^{s</em>{l+1}}<em>{j=1}(\Theta^{(l)}</em>{ji})^2$$<br>Focusing on a single example $x^{(i)},y^{(i)},$ the case of 1 output unit, and ignoring regularization ($\lambda=0$),<br>$$cost(i)\approx y^{(i)}logh_\Theta(x^{(i)})+(1-y^{(i)})logh_\Theta(x^{(i)})$$<br>(Think of $cost(i)\approx(h_\Theta(x^{(i)})-y^{(i)})^2$)<br>l.e. how well is the network doing on example i?<br>$\delta_j^{(l)}$=”error” of cost for $a_j^{(l)}$(unit j in layer l)<br>$\delta_j^{(l)}=\frac{\partial}{\partial z_j^{(l)}}$ for $j \geq 0$, where<br>$$cost(i)= y^{(i)}logh_\Theta(x^{(i)})+(1-y^{(i)})logh_\Theta(x^{(i)})$$  </p>
<h4 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h4><p>参数向量 $\theta$ (e.g. $\theta$ is “unrolled” version of $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$)<br>$\theta=\theta_1,\theta_2,\theta_3…,\theta_n$<br>$\frac{\partial}{\partial\theta_1}J(\theta)\approx\frac{J(\theta_1+\epsilon,\theta_2,\theta_3,..,\theta_n)-J(\theta_1-\epsilon,\theta_2,\theta_3,..,\theta_n)}{2\epsilon}$<br>$\frac{\partial}{\partial\theta_2}J(\theta)\approx\frac{J(\theta_1,\theta_2+\epsilon,\theta_3,..,\theta_n)-J(\theta_1,\theta_2-\epsilon,\theta_3,..,\theta_n)}{2\epsilon}$<br>…<br>$\frac{\partial}{\partial\theta_n}J(\theta)\approx\frac{J(\theta_1,\theta_2,\theta_3,..,\theta_n+\epsilon)-J(\theta_1,\theta_2,\theta_3,..,\theta_n-\epsilon)}{2\epsilon}$<br>检查这种方法计算出来的 GradApprox$\approx$DVec,即可证明我们计算的导数是正确的。  </p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>对于梯度下降法，我们需要对变量$\Theta$给予初始值，如果全部给0的话，在每次迭代之后，权重的大小不会改变，函数式子也会相等，这个神经网络所有的隐藏单元都在计算相同的特征，计算不出什么有趣的东西。<br>解决办法：<br>Initialize each $\Theta^{(l)}_{ij}$ to a random value in $[-\epsilon,\epsilon]$</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>选择一个网络架构（神经元之间的连接模式）<br><img src="/img/machine-learning/34.png" srcset="/img/loading.gif" alt="图34"><br>No.of input units : Dimension of features $x^{(i)}$<br>No.of output units: Number of classes<br>Reasonable defalt: 1 hidden layer, or if &gt;1 hidden layer, have same no.of hidden units in every layer(usually the more the better)<br>训练一个神经网络：  </p>
<ol>
<li>Randomly initial weights</li>
<li>Implement foeward propagation to get $h_\theta(x^{(i)})$ for any $x^{(i)}$</li>
<li>Implement code to compute cost funtion $J(\Theta)$</li>
<li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation vs. using numerical estimate of gradient of $J(\Theta)$.<br>Then disable gradient checking code.  </li>
<li>Use gradient descent or advanced optimization method with backpropagation ti try minimize $J(\Theta)$ as a function of parameters.  </li>
</ol>
<h4 id="无人驾驶"><a href="#无人驾驶" class="headerlink" title="无人驾驶"></a>无人驾驶</h4><p>神经网络应用的一个实例    </p>
<h3 id="应用机器学习的建议"><a href="#应用机器学习的建议" class="headerlink" title="应用机器学习的建议"></a>应用机器学习的建议</h3><p>当要选择或者优化一个机器学习算法的时候，大多数人可能会凭感觉去选择：</p>
<ul>
<li>获取更多训练数据</li>
<li>试试更少的特征量</li>
<li>试试更多的特征</li>
<li>增加多项式</li>
<li>试试增加$\lambda$的值</li>
<li>试试减少$\lambda$的值<br>盲目选择可能会浪费时间。</li>
</ul>
<p>机器学习诊断法：<br>诊断法：您可以运行该测试来了解学习算法在哪些方面有效，哪些无效，以及如何最好地提高其性能的指导。  </p>
<h4 id="评估假设"><a href="#评估假设" class="headerlink" title="评估假设"></a>评估假设</h4><p>判断假设函数是否过拟合  </p>
<ol>
<li>特征量小的时候，画出函数图像  </li>
<li>将数据按照7：3的比例分割成训练集和测试集（乱序）<ul>
<li>Learn parameter $\theta$ from training data</li>
<li>Compute test set error</li>
</ul>
</li>
</ol>
<h4 id="模型选择和训练、验证、测试集"><a href="#模型选择和训练、验证、测试集" class="headerlink" title="模型选择和训练、验证、测试集"></a>模型选择和训练、验证、测试集</h4><p>模型选择  </p>
<ol>
<li>$h_\theta(x)=\theta_0+\theta_1x$</li>
<li>$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$</li>
<li>$h_\theta(x)=\theta_0+\theta_1x+…+\theta_3x^3$<br>.<br>.   </li>
<li>$h_\theta(x)=\theta_0+\theta_1x+…+\theta_{10}x^{10}$</li>
</ol>
<p>拟合这个模型并且估计，这个拟合好的模型假设对新样本的泛化能力。<br>就是取每一个假设和它相应的参数，然后计算出它在测试集中的性能。接下来，为了从这些模型中选出最好的一个，应该看哪个模型有最小的测试误差，但是这样仍然不能公正的评估假设函数的泛化能力，原因在于，我们拟合了一个额外的参数d，也就是多项式的次数，我们用测试集拟合了参数d，我们选择了一个最好地拟合测试集的参数d的值。因此，我们的参数向量在测试集上的性能很可能是对泛化误差过于乐观的估计。（<strong>对测试集的效果好，并不一定对新样本的效果好，并不能证明其泛化能力强弱</strong>）<br>更好的方法：将数据集分成三部分，训练集、交叉验证cv（验证集）和测试集，其比例为6：2：2 。然后我们就可以定义：<br>训练误差（Training error）：<br>$$J_{train}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$<br>交叉验证误差（Cross Validation error）:<br>$$J_{cv}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{cv}^{(i)})-y^{(i)}<em>{cv})^2$$<br>测试误差（Test error）:<br>$$J</em>{test}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{test}^{(i)})-y^{(i)}_{test})^2$$<br>计算出各个假设函数的最小代价函数所对应的参数值，使用交叉验证集来选择出最合适的假设，最后可用测试集来估计假设函数的泛化误差。</p>
<h4 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h4><p>假设你的学习算法表现不好（$J_{cv}(\theta)$ or $J_{test}(\theta)$的值偏大）。这是偏差问题还是方差问题呢？<br><img src="/img/machine-learning/35.png" srcset="/img/loading.gif" alt="图35"><br>Bias高偏差（欠拟合）：  训练集和交叉验证误差都比较大<br>Variance高方差（过拟合）: 训练集误差会小，交叉验证集误差远远大于训练集误差。<br>正则化线性回归<br>模型：<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$$<br>$$J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^m_{(j=1)}\theta_j^2$$<br><img src="/img/machine-learning/36.png" srcset="/img/loading.gif" alt="图36"><br>选择正则化参数$\lambda$<br>模型：<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$$<br>$$J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^m_{(j=1)}\theta_j^2$$</p>
<ol>
<li>Try $\lambda=0$</li>
<li>Try $\lambda=0.01$</li>
<li>Try $\lambda=0.02$</li>
<li>Try $\lambda=0.04$</li>
<li>Try $\lambda=00.08$<br>…</li>
<li>Try $\lambda=10$</li>
</ol>
<p>然后选择有最小交叉验证误差的参数$\lambda$<br><img src="/img/machine-learning/37.png" srcset="/img/loading.gif" alt="图37">  </p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p>绘制学习曲线非常有用，也许想检查你的学习算法运行是否一切正常，或者你希望改进算法的表现。<br>训练误差（Training error）：<br>$$J_{train}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$<br>交叉验证误差（Cross Validation error）:<br>$$J_{cv}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{cv}^{(i)})-y^{(i)}_{cv})^2$$<br>将它们绘制成m相关的函数，10个、20个、30个样本…m个样本。<br><img src="/img/machine-learning/38.png" srcset="/img/loading.gif" alt="图38"><br>当m很小时，对每一个训练样本都能很容易地拟合到很好，所以训练误差将会很小，反过来，当m的值逐渐增大，那么想对每一个训练样本都拟合到很好，就显得愈发困难了。训练集的误差就会越来越大。<br>交叉验证误差就是在没有见过的交叉验证集上的误差。当训练集很小的时候，泛化程度不会很好，意思就是不能很好的适应新样本，因此这个假设就不是一个理想的假设。只有当使用一个足够大的训练集时，才有可能得到一个能够更好拟合数据的假设。 </p>
<p><img src="/img/machine-learning/39.png" srcset="/img/loading.gif" alt="图39"><br>高偏差可以由很高的交叉验证误差和训练误差反映出来。<br>如果学习算法正处于高偏差的情形，那么选用更多的训练集数据对于改善算法无益。<br><img src="/img/machine-learning/40.png" srcset="/img/loading.gif" alt="图40"><br>在高方差的假设函数中，使用更多的训练集数据，对改进算法是有帮助的。  </p>
<ul>
<li>获取更多的训练集数据（改善高方差的问题）</li>
<li>使用更少的特征（对高方差时有效）</li>
<li>使用更多的特征（对高偏差时有效）</li>
<li>增加多项式的特征（修正高偏差）</li>
<li>增加$\lambda$（修正高偏差）</li>
<li>减小$\lambda$ (修正高方差)</li>
</ul>
<h3 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h3><h4 id="确定执行的优先级"><a href="#确定执行的优先级" class="headerlink" title="确定执行的优先级"></a>确定执行的优先级</h4><p>在改进一个算法的时候，有很多的思路，要确定执行改进的顺序。</p>
<h4 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h4><p>推荐路线：</p>
<ul>
<li>从可以快速实现的简单算法开始。 实施它并在您的交叉验证数据上对其进行测试。</li>
<li>绘制学习曲线，以确定是否有可能提供更多数据，更多功能等。</li>
<li>错误分析：手动检查算法在其上出错的示例（在交叉验证集中）。 看看您在任何类型的示例上是否发现任何系统趋势，都会使错误发生。</li>
</ul>
<p>举例：<br>Error Analysis<br>$m_{cv}=500$ 交叉验证集中有500个样本<br>算法错误分类了100封电子邮件<br>手动检查100个错误，并根据以下错误将其分类:<br>(i) 什么类型的邮件<br>(ii) 你认为什么样的特征可以帮助它们被正确分类<br>帮助分类的特征可能有：错误的拼写、奇怪的邮件来源以及垃圾邮件特有的标点符号使用。<br>数值评估的重要性<br>discount/discounts/discounted/discounting应被视为同一个词吗？<br>可以使用“阻止”软件（例如“ Porter stemmer”）<br>错误分析可能无法帮助您确定这是否可能提高性能。 唯一的解决方案是尝试一下，看看它是否有效。<br>需要对有无词干的算法性能进行数值评估（例如，交叉验证误差）。<br>Without stemming:5% error/With stemming:3% error<br>大小写：Mom vd mom<br>多去想和尝试。</p>
<h4 id="不对称性分类的误差评估"><a href="#不对称性分类的误差评估" class="headerlink" title="不对称性分类的误差评估"></a>不对称性分类的误差评估</h4><p>在一个样本中，一个类的数据与另一个类的数据相比多很多。如果你有一个偏斜类，用分类精确度并不能很好地衡量算法，因为你可能获得一个很高的精确度和非常低的误差，但我们不知道它是否是一个好的模型。<br>假设让我们判断病人是否患了癌症。<br><img src="/img/machine-learning/41.png" srcset="/img/loading.gif" alt="图41"><br><em>查准率</em><br>（在我们预测$y=1$的所有患者中，实际上有多少癌症？）<br>$$\frac{True\quad Positives}{Predicted\quad Positive}=\frac{True\quad Positive}{True \quad Positive+False\quad Positive}$$<br><em>召回率</em><br>（在所有实际患有癌症的患者中，我们正确地检测出癌症的比例是多少？）<br>$$\frac{True\quad Positive}{Actull\quad Positive}=\frac{True\quad Positive}{True\quad Positive+False \quad Negative}$$  </p>
<h4 id="精确度和召回率的权衡"><a href="#精确度和召回率的权衡" class="headerlink" title="精确度和召回率的权衡"></a>精确度和召回率的权衡</h4><p>Logistic 回归：$0\leq h_\theta(x)\leq1$<br>Predict 1 if $h_\theta(x)\geq0.5$<br>Predict 0 if $h_\theta(x)&lt; 0.5$<br>假设我们想要预测$y=1$(Cancer) 只有在非常确定的情况下,将0.5修改为0.7,甚至是0.9 。这样会有较高的查准率，较低的召回率。<br>假设我们想避免太多癌症的确诊（避免误诊）。这样会有高的召回率，低的查准率。<br><img src="/img/machine-learning/42.png" srcset="/img/loading.gif" alt="图42"><br>$Average:\frac{P+R}{2}$不可以评估算法的好坏。<br>$F_1Score:2\frac{PR}{P+R}$(F值)  </p>
<h4 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h4><p>大数据基础<br>使用具有很多参数的学习算法（例如具有许多特征的逻辑回归/线性回归;具有许多隐藏单元的神经网络）。</p>
<p>使用大型训练集（不可能过度拟合的情况下）训练出很多参数的假设函数，这可以得到一个高性能的函数。  </p>
<h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>修改 Logistic 回归<br>$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$<br><img src="/img/machine-learning/43.png" srcset="/img/loading.gif" alt="图43"><br>If $y=1$, we want $h_\theta(x)\approx1, \theta^Tx\gg0$<br>If $y=0$, we want $h_\theta(x)\approx1, \theta^Tx\ll0$<br>每个单独的样本对总代价函数的贡献：<br>$$-(ylogh_\theta(x)+(1-y)log(1-h_\theta(x)))=$$<br>$$-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$$<br><img src="/img/machine-learning/44.png" srcset="/img/loading.gif" alt="图44"><br>逻辑回归的代价函数：<br>$$\min\limits_\theta\frac{1}{m}[\sum_{i=1}^my^{(i)}(-logh_\theta(x^{(i)}))+(1-y^{(i)})((-log(1-h_\theta(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$<br>支持向量机：<br>$$\min\limits_\theta C\sum^m_{i=1}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)}cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum^n_{j=1}\theta^2_j$$<br>$$h_\theta(x)=\begin{cases}1&amp;if&amp;\theta^Tx\geq0\0 &amp;otherwise\end{cases}$$</p>
<h4 id="直观上对大间隔的理解"><a href="#直观上对大间隔的理解" class="headerlink" title="直观上对大间隔的理解"></a>直观上对大间隔的理解</h4><p><img src="/img/machine-learning/45.png" srcset="/img/loading.gif" alt="图45"><br>分类器分割的距离称为支持向量机的间距，这使得支持向量机具有鲁棒性。因为它在分离数据时，会尽量用大的间距去分离。因此支持向量机有时会被称为大间距分离器。<br><img src="/img/machine-learning/46.png" srcset="/img/loading.gif" alt="图46"><br>当正则化系数C特别大的时候，分类会变得很敏感。  </p>
<h4 id="大间隔分类器的数学原理"><a href="#大间隔分类器的数学原理" class="headerlink" title="大间隔分类器的数学原理"></a>大间隔分类器的数学原理</h4><p>SVM决策边界<br>$\min\limits_\theta\frac{1}{2}\sum^n_{j=1}\theta^2_j=\frac{1}{2}(\theta_1^2+\theta_2^2)=\frac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})=\frac{1}{2}\parallel\theta\parallel^2$<br>s.t.<br>$$\theta^Tx^{(i)}\geq1 \quad if \quad y^{(i)}=1$$<br>$$\theta^Tx^{(i)}\leq-1 \quad if \quad y^{(i)}=0$$<br>假设$\theta_0=0,n=2$<br><img src="/img/machine-learning/47.png" srcset="/img/loading.gif" alt="图47"><br>$\min\limits_\theta\frac{1}{2}\sum\limits^n_{j=1}\theta^2_j=\frac{1}{2}\parallel\theta\parallel^2$<br>s.t.<br>$$p^{(i)} \cdot \parallel\theta\parallel\geq1\quad if \quad y^{(i)}=1$$<br>$$p^{(i)} \cdot \parallel\theta\parallel\leq-1\quad if \quad y^{(i)}=1$$ </p>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>预测$y=1$ if $\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+…\geq0$<br><img src="/img/machine-learning/48.png" srcset="/img/loading.gif" alt="图48"><br>给出x，计算像素上新的特征点。给定一个实例x，让我将第一个特征$f_1$定义为一种相似度的度量，即度量训练样本x与第一个标记的相似度。<br>$f_1=similarity(x,l^{(1)})=exp(-\frac{\parallel x-l^{(1)}\parallel^2}{2\sigma^2})$<br>该函数称为高斯核函数（相似度函数）。<br>核函数和相似度：<br>$f_1=similarity(x,l^{(1)})=exp(-\frac{\parallel x-l^{(1)}\parallel^2}{2\sigma^2})$<br>如果$x\approx l^{(1)}$:<br>$$f_1\approx exp(-\frac{0^2}{2\sigma^2})\approx 1$$<br>如果$x$ 远离$l^{(1)}$<br>$$f_1\approx exp(-\frac{(large \quad number)^2}{2\sigma^2})\approx 0$$<br><img src="/img/machine-learning/49.png" srcset="/img/loading.gif" alt="图49"><br><img src="/img/machine-learning/50.png" srcset="/img/loading.gif" alt="图50">  </p>
<p>预测$y=1$ 当$\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3\geq0$<br>我们应该怎么得到$l^{(1)},l^{(2)},l^{(3)}…?$<br>SVM with kernels<br>给出$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$<br>选择$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}$<br>给出一个样本x：<br>$f_1=similarity(x,l^{(1)})$<br>$f_2=similarity(x,l^{(2)})$<br>…<br>看样本到各特征点的距离，距离近相似度高核函数数值大，距离远相似度低核函数数值小。<br>假设函数：给出x，计算特征量$f\in\mathbb{R}^{m+1}$<br>预测“y=1”，如果$\theta^Tf\geq0$<br>训练：<br>$$\min\limits_\theta C\sum^m_{i=1}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)}cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum^n_{j=1}\theta^2_j$$<br>SVM参数<br>$C(=\frac{1}{\lambda})$ .<br>Large C:低偏差，高方差。<br>Small C：高偏差，低方差。<br>$\sigma^2$<br>Large $\sigma^2$: 特征量$f_i$变化不多，高偏差，低方差。<br>Small$\sigma^2$: 特征量变化剧烈，低偏差，高方差。  </p>
<h4 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h4><p>使用SVM软件库（例如liblinear,libsvm,..）去解出参数$\theta$，但我们需要去选择参数C和核函数。<br>当我们选择高斯核函数的时候<br>$$f_1= exp(-\frac{(large \quad number)^2}{2\sigma^2})$$<br>需要选择参数$\sigma^2$， 并且注意将特征量归一化，缩放数值比例。</p>
<blockquote>
<p>n=number of features($x\in\mathbb{R}^{n+1}$),m=number of training examples.<br>If n is large(relative to m):<br>Use logistic regression, or SVM without a kernel(“linear kernel”)<br>If n is small, m is intermediate:<br>Use SVM with Gaussian kernel<br>If n is small,m is large:<br>Create/add more features, then use logistic regression or SVM without a kernel.  </p>
</blockquote>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/study/">study</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Python/">Python</a>
                    
                      <a class="hover-with-bg" href="/tags/machine-learning/">machine-learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">吴恩达机器学习（三）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/">
                        <span class="hidden-mobile">吴恩达机器学习（一）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer>
  (function () {
    // 查询存储的记录
    function getRecord(Counter, target) {
      return new Promise(function (resolve, reject) {
        Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({target})))
          .then(resp => resp.json())
          .then(({results, code, error}) => {
            if (code === 401) {
              throw error;
            }
            if (results && results.length > 0) {
              var record = results[0];
              resolve(record);
            } else {
              Counter('post', '/classes/Counter', {target, time: 0})
                .then(resp => resp.json())
                .then((record, error) => {
                  if (error) {
                    throw error;
                  }
                  resolve(record);
                }).catch(error => {
                console.error('Failed to create', error);
                reject(error);
              });
            }
          }).catch((error) => {
          console.error('LeanCloud Counter Error:', error);
          reject(error);
        });
      })
    }

    // 发起自增请求
    function increment(Counter, incrArr) {
      return new Promise(function (resolve, reject) {
        Counter('post', '/batch', {
          "requests": incrArr
        }).then((res) => {
          res = res.json();
          if (res.error) {
            throw res.error;
          }
          resolve(res);
        }).catch((error) => {
          console.error('Failed to save visitor count', error);
          reject(error);
        });
      });
    }

    // 构建自增请求体
    function buildIncrement(objectId) {
      return {
        "method": "PUT",
        "path": `/1.1/classes/Counter/${ objectId }`,
        "body": {
          "time": {
            '__op': 'Increment',
            'amount': 1
          }
        }
      }
    }

    // 校验是否为有效的 UV
    function validUV() {
      var key = 'LeanCloud_UV_Flag';
      var flag = localStorage.getItem(key);
      if (flag) {
        // 距离标记小于 24 小时则不计为 UV
        if (new Date().getTime() - parseInt(flag) <= 86400000) {
          return false;
        }
      }
      localStorage.setItem(key, new Date().getTime().toString());
      return true;
    }

    function addCount(Counter) {
      var enableIncr = 'true' === 'true' && window.location.hostname !== 'localhost';
      var getterArr = [];
      var incrArr = [];

      // 请求 PV 并自增
      var pvCtn = document.querySelector('#leancloud-site-pv-container');
      if (pvCtn || enableIncr) {
        var pvGetter = getRecord(Counter, 'site-pv').then((record) => {
          incrArr.push(buildIncrement(record.objectId))
          var ele = document.querySelector('#leancloud-site-pv');
          if (ele) {
            ele.innerText = record.time + 1;
            if (pvCtn) {
              pvCtn.style.display = 'inline';
            }
          }
        });
        getterArr.push(pvGetter);
      }

      // 请求 UV 并自增
      var uvCtn = document.querySelector('#leancloud-site-uv-container');
      if (uvCtn || enableIncr) {
        var uvGetter = getRecord(Counter, 'site-uv').then((record) => {
          var vuv = validUV();
          vuv && incrArr.push(buildIncrement(record.objectId))
          var ele = document.querySelector('#leancloud-site-uv');
          if (ele) {
            ele.innerText = record.time + (vuv ? 1 : 0);
            if (uvCtn) {
              uvCtn.style.display = 'inline';
            }
          }
        });
        getterArr.push(uvGetter);
      }

      // 如果是文章，请求文章的浏览数，并自增
      if ('true' === 'true') {
        var viewCtn = document.querySelector('#leancloud-post-views-container');
        if (viewCtn || enableIncr) {
          var target = decodeURI('/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/');
          var viewGetter = getRecord(Counter, target).then((record) => {
            incrArr.push(buildIncrement(record.objectId))
            if (viewCtn) {
              var ele = document.querySelector('#leancloud-post-views');
              if (ele) {
                ele.innerText = (record.time || 0) + 1;
                viewCtn.style.display = 'inline';
              }
            }
          });
          getterArr.push(viewGetter);
        }
      }

      // 如果启动计数自增，批量发起自增请求
      if (enableIncr) {
        Promise.all(getterArr).then(() => {
          incrArr.length > 0 && increment(Counter, incrArr);
        })
      }
    }

    var app_id = 'TiyaSIcTbSTjSzxKsX4KmQLG-gzGzoHsz'
    var app_key = 'SIWw0atj9HA0ctavk2YMoTEI'
    var server_url = 'https://tiyasict.lc-cn-n1-shared.com'

    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${ api_server }/1.1${ url }`, {
          method,
          headers: {
            'X-LC-Id': app_id,
            'X-LC-Key': app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };

      addCount(Counter);
    }

    var api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${ app_id.slice(0, 8).toLowerCase() }.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(resp => resp.json())
        .then(({api_server}) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>






  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "吴恩达机器学习（二）&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  













  

  

  

  

  

  






<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"right","hOffset":0,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
