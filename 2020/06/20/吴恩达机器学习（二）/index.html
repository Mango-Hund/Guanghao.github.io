<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>吴恩达机器学习（二） | Liu's blog</title><meta name="description" content="机器学习经典入门"><meta name="keywords" content="Python,machine-learning"><meta name="author" content="Guanghao"><meta name="copyright" content="Guanghao"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="吴恩达机器学习（二）"><meta property="og:url" content="http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta property="og:site_name" content="Liu's blog"><meta property="og:description" content="机器学习经典入门"><meta property="og:image" content="http://lguanghao.com/img/ai.jpg"><meta property="article:published_time" content="2020-06-20T10:45:38.000Z"><meta property="article:modified_time" content="2020-07-25T05:25:04.873Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="吴恩达机器学习（三）" href="http://lguanghao.com/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"><link rel="next" title="吴恩达机器学习（一）" href="http://lguanghao.com/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-07-25 13:25:04'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">18</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#分类"><span class="toc-number">1.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#假设陈述"><span class="toc-number">1.1.</span> <span class="toc-text">假设陈述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#决策界限（decision-boundary）"><span class="toc-number">1.2.</span> <span class="toc-text">决策界限（decision boundary）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代价函数"><span class="toc-number">1.3.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#简化代价函数与梯度下降"><span class="toc-number">1.4.</span> <span class="toc-text">简化代价函数与梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#高级优化"><span class="toc-number">1.5.</span> <span class="toc-text">高级优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多元算法"><span class="toc-number">1.6.</span> <span class="toc-text">多元算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化"><span class="toc-number">2.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#过拟合问题"><span class="toc-number">2.1.</span> <span class="toc-text">过拟合问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#线性回归的正则化"><span class="toc-number">2.2.</span> <span class="toc-text">线性回归的正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Logistic回归的正则化"><span class="toc-number">2.3.</span> <span class="toc-text">Logistic回归的正则化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络学习"><span class="toc-number">3.</span> <span class="toc-text">神经网络学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#非线性假设"><span class="toc-number">3.1.</span> <span class="toc-text">非线性假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经元与大脑"><span class="toc-number">3.2.</span> <span class="toc-text">神经元与大脑</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子与直觉理解"><span class="toc-number">3.3.</span> <span class="toc-text">例子与直觉理解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多元分类"><span class="toc-number">3.4.</span> <span class="toc-text">多元分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络参数的反向传播算法"><span class="toc-number">4.</span> <span class="toc-text">神经网络参数的反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#代价函数-1"><span class="toc-number">4.1.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#反向传播算法"><span class="toc-number">4.2.</span> <span class="toc-text">反向传播算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#理解反向传播算法"><span class="toc-number">4.3.</span> <span class="toc-text">理解反向传播算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度检测"><span class="toc-number">4.4.</span> <span class="toc-text">梯度检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#随机初始化"><span class="toc-number">4.5.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">4.6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#无人驾驶"><span class="toc-number">4.7.</span> <span class="toc-text">无人驾驶</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#应用机器学习的建议"><span class="toc-number">5.</span> <span class="toc-text">应用机器学习的建议</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#评估假设"><span class="toc-number">5.1.</span> <span class="toc-text">评估假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型选择和训练、验证、测试集"><span class="toc-number">5.2.</span> <span class="toc-text">模型选择和训练、验证、测试集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#诊断偏差和方差"><span class="toc-number">5.3.</span> <span class="toc-text">诊断偏差和方差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#学习曲线"><span class="toc-number">5.4.</span> <span class="toc-text">学习曲线</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#机器学习系统设计"><span class="toc-number">6.</span> <span class="toc-text">机器学习系统设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#确定执行的优先级"><span class="toc-number">6.1.</span> <span class="toc-text">确定执行的优先级</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#误差分析"><span class="toc-number">6.2.</span> <span class="toc-text">误差分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#不对称性分类的误差评估"><span class="toc-number">6.3.</span> <span class="toc-text">不对称性分类的误差评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#精确度和召回率的权衡"><span class="toc-number">6.4.</span> <span class="toc-text">精确度和召回率的权衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#机器学习数据"><span class="toc-number">6.5.</span> <span class="toc-text">机器学习数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#支持向量机（SVM）"><span class="toc-number">7.</span> <span class="toc-text">支持向量机（SVM）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优化目标"><span class="toc-number">7.1.</span> <span class="toc-text">优化目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#直观上对大间隔的理解"><span class="toc-number">7.2.</span> <span class="toc-text">直观上对大间隔的理解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#大间隔分类器的数学原理"><span class="toc-number">7.3.</span> <span class="toc-text">大间隔分类器的数学原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#核函数"><span class="toc-number">7.4.</span> <span class="toc-text">核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用SVM"><span class="toc-number">7.5.</span> <span class="toc-text">使用SVM</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/mountains.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Liu's blog</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">吴恩达机器学习（二）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-06-20 18:45:38"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-06-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 13:25:04"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/study/">study</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">7k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 29 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>分类问题，例如<br>Email: 垃圾邮件/不是垃圾邮件<br>Online：信息是真的/信息是假的<br>Tumor：良性的/恶性的<br>$y=\{0,1\}$<br>0:Negative Class负类/1:Positive Class正类<br><img src= "/img/loading.gif" data-src="/img/machine-learning/09.png" alt="图9"><br>将分类器的输出$h_\theta(x)$阀值定位0.5<br>如果$h_\theta\geq0.5$,预测’y=1’<br>如果$h_\theta&lt;0.5$,预测‘y=0’ </p>
<h4 id="假设陈述"><a href="#假设陈述" class="headerlink" title="假设陈述"></a>假设陈述</h4><p>逻辑分类模型<br>我们想要$0\leq h_\theta(x) \leq 1$<br>$h_\theta(x)=g(\theta^Tx)$<br>$g(z)=\frac{1}{1+e^{-z}}$ z是实数 ，g(z)一端趋向1一端趋向0。<br>关于假设h(x)函数的解释：<br>$h_\theta(x)$= 输入x对于y=1的估计概率<br>举例：如果 $x=\begin{bmatrix}x_0 \\ x_1\end{bmatrix}=\begin{bmatrix}1 \\ tumorSize \end{bmatrix}$<br>$h_\theta(x)=0.7$<br>可以说明病人的肿瘤有70%的概率为恶性肿瘤。<br>当特征变量为x，变量的参数为$\theta$，可以表示‘y=1’或‘y=0’的概率为：  </p>
<script type="math/tex; mode=display">h_\theta(x)=P(y=1|x;\theta)</script><script type="math/tex; mode=display">P(y=0|x;\theta)+P(y=1|x;\theta)=1</script><script type="math/tex; mode=display">P(y=0|x;\theta)=1-P(y=1|x;\theta)</script><h4 id="决策界限（decision-boundary）"><a href="#决策界限（decision-boundary）" class="headerlink" title="决策界限（decision boundary）"></a>决策界限（decision boundary）</h4><p>分类函数：<br>$h_\theta(x)=g(\theta^Tx)=P(y=1|x;\theh<br>ta)$<br>$g(z)=\frac{1}{1+e^{-z}}$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/10.png" alt="图10"><br>$g(z)\geq0.5$ when $z \geq 0$。$h_\theta(x)=g(\theta^Tx)\geq0.5$ when $\theta^T\geq0$<br>假设：预测“y=1” 如果$h_\theta(x)\geq0.5$<br>预测“y=0” 如果$h_\theta(x)&lt;0.5$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/11.png" alt="图11"><br>假设$h_\theta(x)=g(\theta_0+\theta x_1+\theta x_2)$,并且已知$\theta^T=\begin{bmatrix}-3 &amp; 1 &amp; 1\end{bmatrix}$<br>可以预测”y=1”, 如果 $-3+x_1+x_2\geq0$，也就是$x_1+x_2\geq3$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/12.png" alt="图12"><br>$h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_4^2)$<br>$\theta^T=\begin{bmatrix}-1 &amp; 0 &amp; 0 &amp; 1 &amp;1\end{bmatrix}$<br>预测“y=1”，如果$-1+x_1^2+x_2^2\geq0$，可改写为$x_1^2+x_2^2\geq0$<br>一旦有了参数$\theta$，决策边界就可以确定了。</p>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>训练集：{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$}<br>m个特征 $x\in\begin{bmatrix}x_0\\x_1\...\\x_n\end{bmatrix}$<br>$x_0=1,y\in\{0,1\}$  </p>
<script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>我们应该怎么去确定参数$\theta$呢？<br>代价函数：<br>线性回归：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum^m_{i=1}\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>平均误差平方可以写为：    </p>
<script type="math/tex; mode=display">Cost(h_\theta(x^{(i)}),y^{(i)})=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>如果直接代入，画出代价函数（优化目标函数如左图，有很多局部最优，且是非凸函数）<br><img src= "/img/loading.gif" data-src="/img/machine-learning/13.png" alt="图13"><br>逻辑回归代价函数</p>
<script type="math/tex; mode=display">Cost(h_\theta(x),y)=\begin{cases}
-log(h_\theta(x)&if&y=1\\
-log(1-h_\theta(x))&if&y=0
\end{cases}</script><p><img src= "/img/loading.gif" data-src="/img/machine-learning/14.png" alt="图14"><br>搞错的代价是巨大的<br><img src= "/img/loading.gif" data-src="/img/machine-learning/15.png" alt="图15">    </p>
<h4 id="简化代价函数与梯度下降"><a href="#简化代价函数与梯度下降" class="headerlink" title="简化代价函数与梯度下降"></a>简化代价函数与梯度下降</h4><p>逻辑回归代价函数（优化目标函数）</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}),y^{(i)})</script><script type="math/tex; mode=display">Cost(h_\theta(x),y)=\begin{cases}
-log(h_\theta(x)&if&y=1\\
-log(1-h_\theta(x))&if&y=0
\end{cases}</script><p>Note:y=0 or 1 always<br>简化$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$<br>由此我们可以得到：  </p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}[\sum^m_{i-1}y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><p>为了拟合出参数$\theta$,我们应该找出让$J(\theta)$取得最小值的参数$\theta$：<br>$\min\limits_\theta J(\theta)$<br>得到了参数，当我们输入一组新的特征去预测时<br>输出：$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$<br>使用梯度下降法来找出代价函数的最小值</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}[\sum^m_{i-1}y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><p>想要得到$\min\limits_\theta J(\theta)$：<br>Repeat{</p>
<script type="math/tex; mode=display">\theta_j:\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)=\theta_j-\alpha\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>(同时更新所有的$\theta_j$)<br>}<br>它与线性回归的参数求解公式一样，但是因为线性回归和逻辑回归所设定的假设函数不一样，一个是$h_\theta(x)=\theta^Tx$,一个是$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$.  </p>
<h4 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h4><p>优化算法<br>代价函数$J(\theta)$,想得到$\min\limits_\theta J(\theta)$<br>当我们输入参数$\theta$的时候，我们可以使用代码计算  </p>
<ul>
<li>$J(\theta)$</li>
<li>$\frac{\partial}{\partial\theta_j}J(\theta)\quad (for\quad j=0,1,2…,n)$<br>梯度下降：<br>Repeat { <script type="math/tex; mode=display">\theta_j:\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)</script>}<br><img src= "/img/loading.gif" data-src="/img/machine-learning/16.png" alt="图16"><br>(共轭梯度法、BFGS、L-BFGS)<br>举例：<br>$\theta=\begin{bmatrix}\theta_1 \\ \theta_2\end{bmatrix}$<br>$J(\theta)=(\theta_1-5)^2+(\theta_2-5)^2$<br>$\frac{\partial}{\partial\theta_1}J(\theta)=2(\theta_1-5)$<br>$\frac{\partial}{\partial\theta_2}J(\theta)=2(\theta_2-5)$    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">funtion[jVal,gradient]</span><br><span class="line">        &#x3D; costFunction(theta)</span><br><span class="line">    jVal &#x3D; (theta(1)-5)^2 + ...</span><br><span class="line">    (theta(2)-5)^2;</span><br><span class="line">    gradient &#x3D; zeros(2,1);</span><br><span class="line">    gradient(1) &#x3D; 2*(theta(1)-5);</span><br><span class="line">    gradient(2) &#x3D; 2*(theta(2)-5);</span><br><span class="line">options &#x3D; optimst(&#39;GradObj&#39;, &#39;on&#39;,&#39;MaxIter&#39;,&#39;100&#39;);</span><br><span class="line">initialTheta &#x3D; zeros(2,1);</span><br><span class="line">[optTheta, functionVal, exitFlag] ...</span><br><span class="line">    &#x3D;fminunc(@costFunction,initialTheta,options)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="多元算法"><a href="#多元算法" class="headerlink" title="多元算法"></a>多元算法</h4><p>多元分类问题<br>Email 标签归类：工作、朋友、家庭和爱好<br>医学诊断：没有生病、感冒、发烧<br>天气：晴朗、多云、下雨、下雪<br><img src= "/img/loading.gif" data-src="/img/machine-learning/17.png" alt="图17"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/18.png" alt="图18"><br>训练一个逻辑回归分类器$h_\theta^{(i)}(x)$对每一个种类进行预测。<br>新输入一个x去做分类预测，取$\max\limits_ih_\theta^{(i)}(x)$即最大值，有最好分类效果的分类器。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/19.png" alt="图19"><br>欠拟合/正常拟合与过拟合<br>如果我们具有太多的特征，则学习的假设可能非常适合训练集，但无法归纳为新的示例（在新示例上预测价格,无法泛化一般化去预测）<br><img src= "/img/loading.gif" data-src="/img/machine-learning/20.png" alt="图20"><br>有两个办法防止过拟合</p>
<ol>
<li>减少特征的数量</li>
</ol>
<ul>
<li>人为的删除某些特征变量</li>
<li>模型选择算法</li>
</ul>
<ol>
<li>正则化</li>
</ol>
<ul>
<li>保留所有的特征变量，但是减少参数值的大小</li>
<li>这个方法非常有效，当我们有很多特征变量时，每个变量都可以对预测做出贡献  </li>
</ul>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/21.png" alt="图21"><br>假设我们在函数中加入惩罚项，使得$\theta_3$,$\theta_4$非常小</p>
<script type="math/tex; mode=display">\min\limits_\theta\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">\min\limits_\theta\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+1000\theta_3^2+1000\theta_4^2</script><p>要想改写后的代价函数尽可能小，那么$\theta_3$,$\theta_4$的值都要尽量接近于0。也就是说我们只需要多余项的参数足够小，这样就可以避免过拟合。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让参数尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 参数 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 参数 的值接近于 0，就像我们忽略了参数值一样。如果我们做到这一点（ 参数 接近 0 ），那么我们将得到一个近似的多项式函数。<br>将参数的值减小，我们会得到一个更简单的假设模型，也更不容易出现过拟合的现象。<br>举例房屋问题：</p>
<ul>
<li>特征：$x_1,x_2,…,x_{100}$</li>
<li>参数：$\theta_0$,$\theta_1$,$\theta_2$,…,$\theta_{100}$ </li>
</ul>
<p>这里有很多特征量，并且我们一时间无法筛查出哪些是不相关的  </p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h(x^{(i)})-y^{(i)})^2</script><p>改写代价函数为(正则化)：</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^n_{j=1}\theta_j^2]</script><p>$\lambda$称为正则化系数，它的作用是控制两个不同目标之间的取舍。<br>在正则化中，我们选择$\theta$去缩小  </p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^n_{j=1}\theta_j^2]</script><p>如果$theta$被设定为一个极大的值，例如$\theta=10^{10}$,参数的值接近于0,可能出现欠拟合的现象。  </p>
<h4 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h4><p>梯度下降法：<br>Repeat  </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>正则化改写成：</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j</script><script type="math/tex; mode=display">\theta_j=:\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>使每次迭代，都使$\theta_j$乘以一个比1略小的数。<br>正规方程法：<br>$X=\begin{bmatrix}(x^{(1)})^T \...\\ (x^{(m)})^T\end{bmatrix}$     $\quad y=\begin{bmatrix}y^{(1)} \...\\ y^{(m)}\end{bmatrix}$  </p>
<script type="math/tex; mode=display">\theta=(X^TX)^{-1}X^Ty</script><p>正则化改写为：  </p>
<script type="math/tex; mode=display">(X^TX+\lambda\begin{bmatrix}0&0&0&...&0 \\ 0&1&0&...&0 \\ 0&0&1&...&0 \\ 0&0&0&...&0 \\ 0&0&0&...&1\end{bmatrix})^{-1}X^Ty</script><h4 id="Logistic回归的正则化"><a href="#Logistic回归的正则化" class="headerlink" title="Logistic回归的正则化"></a>Logistic回归的正则化</h4><p>代价函数：  </p>
<script type="math/tex; mode=display">j(\theta)=-[\frac{1}{m}\sum_{i=1}^my^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><p>正则化改写为：</p>
<script type="math/tex; mode=display">j(\theta)=-[\frac{1}{m}\sum_{i=1}^my^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p>梯度下降法：<br>Repeat  </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>正则化改写成：</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j</script><script type="math/tex; mode=display">\theta_j=:\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><h3 id="神经网络学习"><a href="#神经网络学习" class="headerlink" title="神经网络学习"></a>神经网络学习</h3><h4 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h4><p>对于一般机器学习问题，特征量都会非常多。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/22.png" alt="图22">  </p>
<h4 id="神经元与大脑"><a href="#神经元与大脑" class="headerlink" title="神经元与大脑"></a>神经元与大脑</h4><p>起源：试图模仿大脑的算法。 在80年代和90年代初期被广泛使用； 在90年代后期，人气下降。<br>最近的复兴：适用于许多应用的最新技术.<br><img src= "/img/loading.gif" data-src="/img/machine-learning/23.png" alt="图23"><br>模拟单个神经元<br><img src= "/img/loading.gif" data-src="/img/machine-learning/24.png" alt="图24"><br>模拟神经网络<br><img src= "/img/loading.gif" data-src="/img/machine-learning/25.png" alt="图25"><br>$a_i^{j}=$ “激活值” of unit i in layer j<br>$\Theta^{(j)}=$ 权重控制矩阵从第 j 层到第 j+1 层的映射  </p>
<script type="math/tex; mode=display">a_1^{(2)}=g(\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3)</script><script type="math/tex; mode=display">a_2^{(2)}=g(\Theta^{(1)}_{20}x_0+\Theta^{(1)}_{21}x_1+\Theta^{(1)}_{22}x_2+\Theta^{(1)}_{23}x_3)</script><script type="math/tex; mode=display">a_3^{(2)}=g(\Theta^{(1)}_{30}x_0+\Theta^{(1)}_{31}x_1+\Theta^{(1)}_{32}x_2+\Theta^{(1)}_{33}x_3)</script><script type="math/tex; mode=display">h_\Theta(x)=a_1^{(3)}=g(\Theta^{(2)}_{10}a_0^{(2)}+\Theta^{(2)}_{11}a_1^{(2)}+\Theta^{(2)}_{12}a_2^{(2)}+\Theta^{(2)}_{13}a_3^{(2)})</script><p>如果神经网络在j层有$s_j$个单元，在 j+1 层有$s_{j+1}$个单元，因此$\Theta^{(j)}$将会是一个$s_{j+1}\times(s_j+1)维度的矩阵$。<br>将上述式子进行改写并向量化  </p>
<script type="math/tex; mode=display">a_1^{(2)}=g(z_1^{(2)})</script><script type="math/tex; mode=display">a_2^{(2)}=g(z_2^{(2)})</script><script type="math/tex; mode=display">a_3^{(2)}=g(z_3^{(2)})</script><script type="math/tex; mode=display">
x=\begin{bmatrix}
x_0 \\ x_1 \\ x_2 \\x_3  
\end{bmatrix}
\quad z^{(2)}=\begin{bmatrix}
 z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}
\end{bmatrix}</script><p>$z^{(2)}=\Theta^{(1)}x=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})$<br>Add $a_0^{(2)}=1$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$h_\Theta(x)=a^{(3)}=g(z^{(3)})$<br>上述称为向前传播<br><img src= "/img/loading.gif" data-src="/img/machine-learning/26.png" alt="图26">  </p>
<h4 id="例子与直觉理解"><a href="#例子与直觉理解" class="headerlink" title="例子与直觉理解"></a>例子与直觉理解</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/27.png" alt="图27"><br>Simple example:AND<br>$x_1,x_2\in\{0,1\}$<br>$y=x_1\quad AND\quad x_2$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/28.png" alt="图28"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/29.png" alt="图29"><br>隐藏层可以计算一些，然后下一层计算更复杂的，层数越多，计算越复杂，最后这些特征作用于输出。  </p>
<h4 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h4><p>要在神经网络中实现多类别分类，采用的方法本质上是一种一对多法的扩展。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/30.png" alt="图30">   </p>
<h3 id="神经网络参数的反向传播算法"><a href="#神经网络参数的反向传播算法" class="headerlink" title="神经网络参数的反向传播算法"></a>神经网络参数的反向传播算法</h3><h4 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/31.png" alt="图31"><br>二元分类：<br>$y=0/1$<br>1个输出结果<br>多元分类：<br>$y=\mathbb{R}^K \quad E.g.\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 1 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 0 \\ 1 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 0 \\ 0 \\ 1\end{bmatrix}$<br>K个输出。<br>逻辑回归的代价函数  </p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta_j^2</script><p>神经网络：<br>$h_\Theta(x)\in\mathbb{R}^K,(h_\Theta(x))_i=i^{th}output$  </p>
<script type="math/tex; mode=display">j(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta^{(l)}_{ji})^2</script><h4 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h4><script type="math/tex; mode=display">j(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta^{(l)}_{ji})^2</script><p>目的是$\min_\Theta J(\Theta)$<br>我们需要计算</p>
<ul>
<li>$J(\Theta)$</li>
<li>$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$  </li>
</ul>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/32.png" alt="图32"><br>给一个训练样板（x，y）：<br>按照下面的顺序进行计算：<br>$a^{(1)}=x$<br>$z{(2)}=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})\quad (add\quad a_0^{(2)})$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$a^{(3)}=g(z^{(3)})\quad (add\quad a_0^{(3)})$<br>$z^{(4)}=\Theta^{(3)}a^{(3)}$<br>$a^{(4)}=h_\Theta(x)=g(z^{(4)})$<br>误差：$\delta^{(l)}_j=$ 第l层第j个节点的误差。<br>例如对于每一个输出（层数L=4）  </p>
<script type="math/tex; mode=display">\delta_j^{(4)}=a_j^{4}-y_j</script><script type="math/tex; mode=display">\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*g'(z^{(3)})</script><script type="math/tex; mode=display">\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*g'(z^{(3)})</script><p>推导见<br><a href="https://zhuanlan.zhihu.com/p/25609953" target="_blank" rel="noopener">神经网络反向传播算法详细推导</a><br>反向传播算法：<br>训练集$\{(x^{(1)},y^{(1)}),…,(x^{(m)},y^{(m)})\}$<br>设$\Delta^{(l)}_{ij}=0(对于全部的l,i,j来说)$<br>For i=1 to m<br>Set $a^{(1)}=x^{(i)}$<br>Perform forward propagation to compute $a^{(l)}$ for l = 2,3,…,L.<br>Using $y^{(i)}$,compute $\delta^{(L)}=a^{(L)}-y^{(i)}$<br>Compute $\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$<br>$\Delta^{(l)}_{ij}:=\Delta^{(l)}_{ij}+a_j^{(l)}\delta_i^{(l+1)}$<br>我们可以得到<br>$D_{ij}^{(l)}:=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda\Theta_{ij}^{(l)}$ if $j\neq 0$<br>$D_{ij}^{(l)}:=\frac{1}{m}\Delta^{(l)}_{ij}$ if $j= 0$<br>则有：$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)=D_{ij}^{(l)}$</p>
<h4 id="理解反向传播算法"><a href="#理解反向传播算法" class="headerlink" title="理解反向传播算法"></a>理解反向传播算法</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/33.png" alt="图33"><br>反向传播算法做了什么工作？  </p>
<script type="math/tex; mode=display">j(\Theta)=-\frac{1}{m}[\sum_{i=1}^m\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta^{(l)}_{ji})^2</script><p>Focusing on a single example $x^{(i)},y^{(i)},$ the case of 1 output unit, and ignoring regularization ($\lambda=0$),</p>
<script type="math/tex; mode=display">cost(i)\approx y^{(i)}logh_\Theta(x^{(i)})+(1-y^{(i)})logh_\Theta(x^{(i)})</script><p>(Think of $cost(i)\approx(h_\Theta(x^{(i)})-y^{(i)})^2$)<br>l.e. how well is the network doing on example i?<br>$\delta_j^{(l)}$=”error” of cost for $a_j^{(l)}$(unit j in layer l)<br>$\delta_j^{(l)}=\frac{\partial}{\partial z_j^{(l)}}$ for $j \geq 0$, where </p>
<script type="math/tex; mode=display">cost(i)= y^{(i)}logh_\Theta(x^{(i)})+(1-y^{(i)})logh_\Theta(x^{(i)})</script><h4 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h4><p>参数向量 $\theta$ (e.g. $\theta$ is “unrolled” version of $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$)<br>$\theta=\theta_1,\theta_2,\theta_3…,\theta_n$<br>$\frac{\partial}{\partial\theta_1}J(\theta)\approx\frac{J(\theta_1+\epsilon,\theta_2,\theta_3,..,\theta_n)-J(\theta_1-\epsilon,\theta_2,\theta_3,..,\theta_n)}{2\epsilon}$<br>$\frac{\partial}{\partial\theta_2}J(\theta)\approx\frac{J(\theta_1,\theta_2+\epsilon,\theta_3,..,\theta_n)-J(\theta_1,\theta_2-\epsilon,\theta_3,..,\theta_n)}{2\epsilon}$<br>…<br>$\frac{\partial}{\partial\theta_n}J(\theta)\approx\frac{J(\theta_1,\theta_2,\theta_3,..,\theta_n+\epsilon)-J(\theta_1,\theta_2,\theta_3,..,\theta_n-\epsilon)}{2\epsilon}$<br>检查这种方法计算出来的 GradApprox$\approx$DVec,即可证明我们计算的导数是正确的。  </p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>对于梯度下降法，我们需要对变量$\Theta$给予初始值，如果全部给0的话，在每次迭代之后，权重的大小不会改变，函数式子也会相等，这个神经网络所有的隐藏单元都在计算相同的特征，计算不出什么有趣的东西。<br>解决办法：<br>Initialize each $\Theta^{(l)}_{ij}$ to a random value in $[-\epsilon,\epsilon]$</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>选择一个网络架构（神经元之间的连接模式）<br><img src= "/img/loading.gif" data-src="/img/machine-learning/34.png" alt="图34"><br>No.of input units : Dimension of features $x^{(i)}$<br>No.of output units: Number of classes<br>Reasonable defalt: 1 hidden layer, or if &gt;1 hidden layer, have same no.of hidden units in every layer(usually the more the better)<br>训练一个神经网络：  </p>
<ol>
<li>Randomly initial weights</li>
<li>Implement foeward propagation to get $h_\theta(x^{(i)})$ for any $x^{(i)}$</li>
<li>Implement code to compute cost funtion $J(\Theta)$</li>
<li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation vs. using numerical estimate of gradient of $J(\Theta)$.<br>Then disable gradient checking code.  </li>
<li>Use gradient descent or advanced optimization method with backpropagation ti try minimize $J(\Theta)$ as a function of parameters.  </li>
</ol>
<h4 id="无人驾驶"><a href="#无人驾驶" class="headerlink" title="无人驾驶"></a>无人驾驶</h4><p>神经网络应用的一个实例    </p>
<h3 id="应用机器学习的建议"><a href="#应用机器学习的建议" class="headerlink" title="应用机器学习的建议"></a>应用机器学习的建议</h3><p>当要选择或者优化一个机器学习算法的时候，大多数人可能会凭感觉去选择：</p>
<ul>
<li>获取更多训练数据</li>
<li>试试更少的特征量</li>
<li>试试更多的特征</li>
<li>增加多项式</li>
<li>试试增加$\lambda$的值</li>
<li>试试减少$\lambda$的值<br>盲目选择可能会浪费时间。</li>
</ul>
<p>机器学习诊断法：<br>诊断法：您可以运行该测试来了解学习算法在哪些方面有效，哪些无效，以及如何最好地提高其性能的指导。  </p>
<h4 id="评估假设"><a href="#评估假设" class="headerlink" title="评估假设"></a>评估假设</h4><p>判断假设函数是否过拟合  </p>
<ol>
<li>特征量小的时候，画出函数图像  </li>
<li>将数据按照7：3的比例分割成训练集和测试集（乱序）<ul>
<li>Learn parameter $\theta$ from training data</li>
<li>Compute test set error</li>
</ul>
</li>
</ol>
<h4 id="模型选择和训练、验证、测试集"><a href="#模型选择和训练、验证、测试集" class="headerlink" title="模型选择和训练、验证、测试集"></a>模型选择和训练、验证、测试集</h4><p>模型选择  </p>
<ol>
<li>$h_\theta(x)=\theta_0+\theta_1x$</li>
<li>$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$</li>
<li>$h_\theta(x)=\theta_0+\theta_1x+…+\theta_3x^3$<br>.<br>.   </li>
<li>$h_\theta(x)=\theta_0+\theta_1x+…+\theta_{10}x^{10}$</li>
</ol>
<p>拟合这个模型并且估计，这个拟合好的模型假设对新样本的泛化能力。<br>就是取每一个假设和它相应的参数，然后计算出它在测试集中的性能。接下来，为了从这些模型中选出最好的一个，应该看哪个模型有最小的测试误差，但是这样仍然不能公正的评估假设函数的泛化能力，原因在于，我们拟合了一个额外的参数d，也就是多项式的次数，我们用测试集拟合了参数d，我们选择了一个最好地拟合测试集的参数d的值。因此，我们的参数向量在测试集上的性能很可能是对泛化误差过于乐观的估计。（<strong>对测试集的效果好，并不一定对新样本的效果好，并不能证明其泛化能力强弱</strong>）<br>更好的方法：将数据集分成三部分，训练集、交叉验证cv（验证集）和测试集，其比例为6：2：2 。然后我们就可以定义：<br>训练误差（Training error）：</p>
<script type="math/tex; mode=display">J_{train}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>交叉验证误差（Cross Validation error）:</p>
<script type="math/tex; mode=display">J_{cv}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{cv}^{(i)})-y^{(i)}_{cv})^2</script><p>测试误差（Test error）:</p>
<script type="math/tex; mode=display">J_{test}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{test}^{(i)})-y^{(i)}_{test})^2</script><p>计算出各个假设函数的最小代价函数所对应的参数值，使用交叉验证集来选择出最合适的假设，最后可用测试集来估计假设函数的泛化误差。</p>
<h4 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h4><p>假设你的学习算法表现不好（$J_{cv}(\theta)$ or $J_{test}(\theta)$的值偏大）。这是偏差问题还是方差问题呢？<br><img src= "/img/loading.gif" data-src="/img/machine-learning/35.png" alt="图35"><br>Bias高偏差（欠拟合）：  训练集和交叉验证误差都比较大<br>Variance高方差（过拟合）: 训练集误差会小，交叉验证集误差远远大于训练集误差。<br>正则化线性回归<br>模型：</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4</script><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^m_{(j=1)}\theta_j^2</script><p><img src= "/img/loading.gif" data-src="/img/machine-learning/36.png" alt="图36"><br>选择正则化参数$\lambda$<br>模型：</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4</script><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^m_{(j=1)}\theta_j^2</script><ol>
<li>Try $\lambda=0$</li>
<li>Try $\lambda=0.01$</li>
<li>Try $\lambda=0.02$</li>
<li>Try $\lambda=0.04$</li>
<li>Try $\lambda=00.08$<br>…</li>
<li>Try $\lambda=10$</li>
</ol>
<p>然后选择有最小交叉验证误差的参数$\lambda$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/37.png" alt="图37">  </p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p>绘制学习曲线非常有用，也许想检查你的学习算法运行是否一切正常，或者你希望改进算法的表现。<br>训练误差（Training error）：</p>
<script type="math/tex; mode=display">J_{train}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>交叉验证误差（Cross Validation error）:</p>
<script type="math/tex; mode=display">J_{cv}(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x_{cv}^{(i)})-y^{(i)}_{cv})^2</script><p>将它们绘制成m相关的函数，10个、20个、30个样本…m个样本。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/38.png" alt="图38"><br>当m很小时，对每一个训练样本都能很容易地拟合到很好，所以训练误差将会很小，反过来，当m的值逐渐增大，那么想对每一个训练样本都拟合到很好，就显得愈发困难了。训练集的误差就会越来越大。<br>交叉验证误差就是在没有见过的交叉验证集上的误差。当训练集很小的时候，泛化程度不会很好，意思就是不能很好的适应新样本，因此这个假设就不是一个理想的假设。只有当使用一个足够大的训练集时，才有可能得到一个能够更好拟合数据的假设。 </p>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/39.png" alt="图39"><br>高偏差可以由很高的交叉验证误差和训练误差反映出来。<br>如果学习算法正处于高偏差的情形，那么选用更多的训练集数据对于改善算法无益。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/40.png" alt="图40"><br>在高方差的假设函数中，使用更多的训练集数据，对改进算法是有帮助的。  </p>
<ul>
<li>获取更多的训练集数据（改善高方差的问题）</li>
<li>使用更少的特征（对高方差时有效）</li>
<li>使用更多的特征（对高偏差时有效）</li>
<li>增加多项式的特征（修正高偏差）</li>
<li>增加$\lambda$（修正高偏差）</li>
<li>减小$\lambda$ (修正高方差)</li>
</ul>
<h3 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h3><h4 id="确定执行的优先级"><a href="#确定执行的优先级" class="headerlink" title="确定执行的优先级"></a>确定执行的优先级</h4><p>在改进一个算法的时候，有很多的思路，要确定执行改进的顺序。</p>
<h4 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h4><p>推荐路线：</p>
<ul>
<li>从可以快速实现的简单算法开始。 实施它并在您的交叉验证数据上对其进行测试。</li>
<li>绘制学习曲线，以确定是否有可能提供更多数据，更多功能等。</li>
<li>错误分析：手动检查算法在其上出错的示例（在交叉验证集中）。 看看您在任何类型的示例上是否发现任何系统趋势，都会使错误发生。</li>
</ul>
<p>举例：<br>Error Analysis<br>$m_{cv}=500$ 交叉验证集中有500个样本<br>算法错误分类了100封电子邮件<br>手动检查100个错误，并根据以下错误将其分类:<br>(i) 什么类型的邮件<br>(ii) 你认为什么样的特征可以帮助它们被正确分类<br>帮助分类的特征可能有：错误的拼写、奇怪的邮件来源以及垃圾邮件特有的标点符号使用。<br>数值评估的重要性<br>discount/discounts/discounted/discounting应被视为同一个词吗？<br>可以使用“阻止”软件（例如“ Porter stemmer”）<br>错误分析可能无法帮助您确定这是否可能提高性能。 唯一的解决方案是尝试一下，看看它是否有效。<br>需要对有无词干的算法性能进行数值评估（例如，交叉验证误差）。<br>Without stemming:5% error/With stemming:3% error<br>大小写：Mom vd mom<br>多去想和尝试。</p>
<h4 id="不对称性分类的误差评估"><a href="#不对称性分类的误差评估" class="headerlink" title="不对称性分类的误差评估"></a>不对称性分类的误差评估</h4><p>在一个样本中，一个类的数据与另一个类的数据相比多很多。如果你有一个偏斜类，用分类精确度并不能很好地衡量算法，因为你可能获得一个很高的精确度和非常低的误差，但我们不知道它是否是一个好的模型。<br>假设让我们判断病人是否患了癌症。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/41.png" alt="图41"><br><em>查准率</em><br>（在我们预测$y=1$的所有患者中，实际上有多少癌症？） </p>
<script type="math/tex; mode=display">\frac{True\quad Positives}{Predicted\quad Positive}=\frac{True\quad Positive}{True \quad Positive+False\quad Positive}</script><p><em>召回率</em><br>（在所有实际患有癌症的患者中，我们正确地检测出癌症的比例是多少？） </p>
<script type="math/tex; mode=display">\frac{True\quad Positive}{Actull\quad Positive}=\frac{True\quad Positive}{True\quad Positive+False \quad Negative}</script><h4 id="精确度和召回率的权衡"><a href="#精确度和召回率的权衡" class="headerlink" title="精确度和召回率的权衡"></a>精确度和召回率的权衡</h4><p>Logistic 回归：$0\leq h_\theta(x)\leq1$<br>Predict 1 if $h_\theta(x)\geq0.5$<br>Predict 0 if $h_\theta(x)&lt; 0.5$<br>假设我们想要预测$y=1$(Cancer) 只有在非常确定的情况下,将0.5修改为0.7,甚至是0.9 。这样会有较高的查准率，较低的召回率。<br>假设我们想避免太多癌症的确诊（避免误诊）。这样会有高的召回率，低的查准率。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/42.png" alt="图42"><br>$Average:\frac{P+R}{2}$不可以评估算法的好坏。<br>$F_1Score:2\frac{PR}{P+R}$(F值)  </p>
<h4 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h4><p>大数据基础<br>使用具有很多参数的学习算法（例如具有许多特征的逻辑回归/线性回归;具有许多隐藏单元的神经网络）。</p>
<p>使用大型训练集（不可能过度拟合的情况下）训练出很多参数的假设函数，这可以得到一个高性能的函数。  </p>
<h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>修改 Logistic 回归<br>$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/43.png" alt="图43"><br>If $y=1$, we want $h_\theta(x)\approx1, \theta^Tx\gg0$<br>If $y=0$, we want $h_\theta(x)\approx1, \theta^Tx\ll0$<br>每个单独的样本对总代价函数的贡献：</p>
<script type="math/tex; mode=display">-(ylogh_\theta(x)+(1-y)log(1-h_\theta(x)))=</script><script type="math/tex; mode=display">-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})</script><p><img src= "/img/loading.gif" data-src="/img/machine-learning/44.png" alt="图44"><br>逻辑回归的代价函数：  </p>
<script type="math/tex; mode=display">\min\limits_\theta\frac{1}{m}[\sum_{i=1}^my^{(i)}(-logh_\theta(x^{(i)}))+(1-y^{(i)})((-log(1-h_\theta(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p>支持向量机：  </p>
<script type="math/tex; mode=display">\min\limits_\theta C\sum^m_{i=1}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)}cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum^n_{j=1}\theta^2_j</script><script type="math/tex; mode=display">h_\theta(x)=\begin{cases}1&if&\theta^Tx\geq0\\0 &otherwise\end{cases}</script><h4 id="直观上对大间隔的理解"><a href="#直观上对大间隔的理解" class="headerlink" title="直观上对大间隔的理解"></a>直观上对大间隔的理解</h4><p><img src= "/img/loading.gif" data-src="/img/machine-learning/45.png" alt="图45"><br>分类器分割的距离称为支持向量机的间距，这使得支持向量机具有鲁棒性。因为它在分离数据时，会尽量用大的间距去分离。因此支持向量机有时会被称为大间距分离器。<br><img src= "/img/loading.gif" data-src="/img/machine-learning/46.png" alt="图46"><br>当正则化系数C特别大的时候，分类会变得很敏感。  </p>
<h4 id="大间隔分类器的数学原理"><a href="#大间隔分类器的数学原理" class="headerlink" title="大间隔分类器的数学原理"></a>大间隔分类器的数学原理</h4><p>SVM决策边界<br>$\min\limits_\theta\frac{1}{2}\sum^n_{j=1}\theta^2_j=\frac{1}{2}(\theta_1^2+\theta_2^2)=\frac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})=\frac{1}{2}\parallel\theta\parallel^2$<br>s.t.  </p>
<script type="math/tex; mode=display">\theta^Tx^{(i)}\geq1 \quad if \quad y^{(i)}=1</script><script type="math/tex; mode=display">\theta^Tx^{(i)}\leq-1 \quad if \quad y^{(i)}=0</script><p>假设$\theta_0=0,n=2$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/47.png" alt="图47"><br>$\min\limits_\theta\frac{1}{2}\sum\limits^n_{j=1}\theta^2_j=\frac{1}{2}\parallel\theta\parallel^2$<br>s.t.  </p>
<script type="math/tex; mode=display">p^{(i)} \cdot \parallel\theta\parallel\geq1\quad if \quad y^{(i)}=1</script><script type="math/tex; mode=display">p^{(i)} \cdot \parallel\theta\parallel\leq-1\quad if \quad y^{(i)}=1</script><h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>预测$y=1$ if $\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+…\geq0$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/48.png" alt="图48"><br>给出x，计算像素上新的特征点。给定一个实例x，让我将第一个特征$f_1$定义为一种相似度的度量，即度量训练样本x与第一个标记的相似度。<br>$f_1=similarity(x,l^{(1)})=exp(-\frac{\parallel x-l^{(1)}\parallel^2}{2\sigma^2})$<br>该函数称为高斯核函数（相似度函数）。<br>核函数和相似度：<br>$f_1=similarity(x,l^{(1)})=exp(-\frac{\parallel x-l^{(1)}\parallel^2}{2\sigma^2})$<br>如果$x\approx l^{(1)}$:  </p>
<script type="math/tex; mode=display">f_1\approx exp(-\frac{0^2}{2\sigma^2})\approx 1</script><p>如果$x$ 远离$l^{(1)}$</p>
<script type="math/tex; mode=display">f_1\approx exp(-\frac{(large \quad number)^2}{2\sigma^2})\approx 0</script><p><img src= "/img/loading.gif" data-src="/img/machine-learning/49.png" alt="图49"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/50.png" alt="图50">  </p>
<p>预测$y=1$ 当$\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3\geq0$<br>我们应该怎么得到$l^{(1)},l^{(2)},l^{(3)}…?$<br>SVM with kernels<br>给出$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$<br>选择$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}$<br>给出一个样本x：<br>$f_1=similarity(x,l^{(1)})$<br>$f_2=similarity(x,l^{(2)})$<br>…<br>看样本到各特征点的距离，距离近相似度高核函数数值大，距离远相似度低核函数数值小。<br>假设函数：给出x，计算特征量$f\in\mathbb{R}^{m+1}$<br>预测“y=1”，如果$\theta^Tf\geq0$<br>训练：  </p>
<script type="math/tex; mode=display">\min\limits_\theta C\sum^m_{i=1}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)}cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum^n_{j=1}\theta^2_j</script><p>SVM参数<br>$C(=\frac{1}{\lambda})$ .<br>Large C:低偏差，高方差。<br>Small C：高偏差，低方差。<br>$\sigma^2$<br>Large $\sigma^2$: 特征量$f_i$变化不多，高偏差，低方差。<br>Small$\sigma^2$: 特征量变化剧烈，低偏差，高方差。  </p>
<h4 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h4><p>使用SVM软件库（例如liblinear,libsvm,..）去解出参数$\theta$，但我们需要去选择参数C和核函数。<br>当我们选择高斯核函数的时候</p>
<script type="math/tex; mode=display">f_1= exp(-\frac{(large \quad number)^2}{2\sigma^2})</script><p>需要选择参数$\sigma^2$， 并且注意将特征量归一化，缩放数值比例。</p>
<blockquote>
<p>n=number of features($x\in\mathbb{R}^{n+1}$),m=number of training examples.<br>If n is large(relative to m):<br>Use logistic regression, or SVM without a kernel(“linear kernel”)<br>If n is small, m is intermediate:<br>Use SVM with Gaussian kernel<br>If n is small,m is large:<br>Create/add more features, then use logistic regression or SVM without a kernel.  </p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Guanghao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/">http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lguanghao.com" target="_blank">Liu's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/machine-learning/">machine-learning</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"><img class="prev-cover" data-src="/img/ai.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">吴恩达机器学习（三）</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"><img class="next-cover" data-src="/img/ai.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">吴恩达机器学习（一）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/07/22/Jupyter notebook快速上手/" title="Jupyter-notebook"><img class="relatedPosts_cover" data-src="/img/jupyter.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-22</div><div class="relatedPosts_title">Jupyter-notebook</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/02/吴恩达机器学习（三）/" title="吴恩达机器学习（三）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="relatedPosts_title">吴恩达机器学习（三）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/10/吴恩达机器学习（一）/" title="吴恩达机器学习（一）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-10</div><div class="relatedPosts_title">吴恩达机器学习（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/20/数据分析（三）/" title="数据分析工具（三）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-20</div><div class="relatedPosts_title">数据分析工具（三）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/13/数据分析（二）/" title="数据分析工具（二）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-13</div><div class="relatedPosts_title">数据分析工具（二）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/03/数据分析（一）/" title="数据分析工具（一）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-03</div><div class="relatedPosts_title">数据分析工具（一）</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(/img/mountains.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By Guanghao</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"right","hOffset":0,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body></html>