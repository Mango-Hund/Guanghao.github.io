<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>吴恩达机器学习（一） | Liu's blog</title><meta name="description" content="机器学习经典入门"><meta name="keywords" content="Python,machine-learning"><meta name="author" content="Guanghao"><meta name="copyright" content="Guanghao"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://lguanghao.com/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="吴恩达机器学习（一）"><meta property="og:url" content="http://lguanghao.com/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"><meta property="og:site_name" content="Liu's blog"><meta property="og:description" content="机器学习经典入门"><meta property="og:image" content="http://lguanghao.com/img/ai.jpg"><meta property="article:published_time" content="2020-06-10T11:45:38.000Z"><meta property="article:modified_time" content="2020-07-25T05:25:04.873Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="吴恩达机器学习（二）" href="http://lguanghao.com/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="next" title="数据分析工具（三）" href="http://lguanghao.com/2020/03/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-07-25 13:25:04'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">24</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-欢迎"><span class="toc-number">1.</span> <span class="toc-text">1 欢迎</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-什么是机器学习"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 什么是机器学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-单变量线性回归"><span class="toc-number">2.</span> <span class="toc-text">2 单变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-代价函数"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-梯度下降"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-线性回归的梯度下降"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 线性回归的梯度下降</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-线性代数回顾"><span class="toc-number">3.</span> <span class="toc-text">3 线性代数回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-矩阵和向量"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 矩阵和向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-加法和标量乘法"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 加法和标量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-矩阵向量乘法"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 矩阵向量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-矩阵乘法"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-矩阵乘法的特征"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 矩阵乘法的特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-逆和转置"><span class="toc-number">3.6.</span> <span class="toc-text">3.6 逆和转置</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-多变量线性回归"><span class="toc-number">4.</span> <span class="toc-text">4 多变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-多功能"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 多功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-多元梯度下降法"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 多元梯度下降法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-特征和多项目式回归"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 特征和多项目式回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-正规方程"><span class="toc-number">5.</span> <span class="toc-text">5 正规方程</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/mountains.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Liu's blog</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">吴恩达机器学习（一）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-06-10 19:45:38"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-06-10</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-25 13:25:04"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/study/">study</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 8 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="1-欢迎"><a href="#1-欢迎" class="headerlink" title="1 欢迎"></a>1 欢迎</h3><h4 id="1-1-什么是机器学习"><a href="#1-1-什么是机器学习" class="headerlink" title="1.1 什么是机器学习"></a>1.1 什么是机器学习</h4><p> A computer program is said to learn from experience E with respect  to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.<br>Supervised learning 监督学习<br>监督学习是指我们给算法一个数据集，其中包含了正确的答案（回归问题、分类问题 ）<br>Unsupervisef learning  无监督学习<br>聚类算法：探究数据集中的数据结构，数据是没有什么标签的，将数据进行归类<br>强化学习：是指智能系统在与环境的连续互动中学习最优行为测略的机器学习问题，本质是学习最优打序管决策<br><img src= "/img/loading.gif" data-src="/img/machine-learning/01.png" alt="图1">  </p>
<h3 id="2-单变量线性回归"><a href="#2-单变量线性回归" class="headerlink" title="2 单变量线性回归"></a>2 单变量线性回归</h3><h4 id="2-1-代价函数"><a href="#2-1-代价函数" class="headerlink" title="2.1 代价函数"></a>2.1 代价函数</h4><script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x</script><p>把$\theta$称为模型参数 Parameter<br>假设函数不同，参数也不同<br>在线性回归中，训练集给出一些点，我们要选择最能拟合的假设函数，合理预测y值</p>
<script type="math/tex; mode=display">\sum^m_{i=1}(h_\theta(x^{(i)-y^{(i)}}))^2</script><p>预测值和实际值的差的平方误差和或者说预测价格与实际卖出价格的差的平方。m是训练集的容量。<br>代价函数也称平均误差函数，可写作</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)-y^{(i)}}))^2</script><p>我们要做的是关于$\theta_0$和$\theta_1$对函数$J(\theta_0,\theta_1)$求最小值。<br>Hypothesis: $h_\theta(x)=\theta_0+\theta_1x$<br>Parameters: $\theta_0,\theta_1$<br>Cost Function: $J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$<br>Goal: $minimize_{\theta_0,\theta_1} J(\theta_0,\theta_1)$<br>代价函数=优化目标<br><img src= "/img/loading.gif" data-src="/img/machine-learning/02.png" alt="图2"><br><img src= "/img/loading.gif" data-src="/img/machine-learning/03.png" alt="图3">   </p>
<h4 id="2-2-梯度下降"><a href="#2-2-梯度下降" class="headerlink" title="2.2 梯度下降"></a>2.2 梯度下降</h4><p>用梯度下降算法最小化任意函数J<br>Have some function $J(\theta_0,\theta_1)$<br>Want $min{J(\theta_0,\theta_1})$<br><strong>Outline:</strong></p>
<ul>
<li>Start with some $\theta_0,\theta_1$</li>
<li>Keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at a minimum</li>
</ul>
<p><img src= "/img/loading.gif" data-src="/img/machine-learning/04.png" alt="图4"><br>$\theta_0$和$\theta_1$在水平轴上，函数J在垂直坐标轴上，图形表面高度则是J的值。我们从$\theta_0$和$\theta_1$的某个值出发，对其两个赋初值，从这个函数表面的某个点出发。<br>Gradient descent algorithm<br>repeat until convergence{ </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad(for\quad j=0 \quad and \quad j=1)</script><p>}  </p>
<hr>
<p>Corret: Simultaneous update<br>$temp0:=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)$<br>$temp1:=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)$<br>$\theta_0:=temp0$<br>$\theta_1:=temp1$<br>用“:=”表示赋值，是一个赋值运算符，这里的$\alpha$是一个被称为学习率的数字，用来控制，梯度下降时，我们迈出多大的步子。<br>我们要同时更新$\theta_0,\theta_1$，同步更新<br>导数项代表该点的斜率<br><img src= "/img/loading.gif" data-src="/img/machine-learning/05.png" alt="图5"><br>$\theta_1:=\theta_1\alpha\frac{\partial}{\partial\theta_1}J(\theta_1)$<br>If $\alpha$ is too small,gradient descent can be slow.<br>If $\alpha$ is too large,gradient descent can overshoot the minimum. It may fail to converge, or even diverge.</p>
<h4 id="2-3-线性回归的梯度下降"><a href="#2-3-线性回归的梯度下降" class="headerlink" title="2.3 线性回归的梯度下降"></a>2.3 线性回归的梯度下降</h4><p>将梯度函数和代价函数结合得到线性回归的算法<br>Gradient descent algorithm<br>repeat until convergence{ </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad(for\quad j=0 \quad and \quad j=1)</script><p>}<br>Linear Regression Model<br>Hypothesis:</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x</script><p>Cost Function: </p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>关键问题在导数项：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) = \frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)-y^{(i)}}))^2=\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum^m_{i=1}(\theta_0+\theta_1x^{i}-y^{i})^2</script><script type="math/tex; mode=display">j=0 : \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">j=1 : \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}</script><p>不断重复同步更新：<br>repeat until convergence</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">\theta_1=\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}</script><p>我们学习出的算法叫做：Batch梯度下降法 </p>
<h3 id="3-线性代数回顾"><a href="#3-线性代数回顾" class="headerlink" title="3 线性代数回顾"></a>3 线性代数回顾</h3><h4 id="3-1-矩阵和向量"><a href="#3-1-矩阵和向量" class="headerlink" title="3.1 矩阵和向量"></a>3.1 矩阵和向量</h4><p>矩阵：由数字组成的阵列，并写在方括号里<br>矩阵的维数应该写作：行<em>列<br>向量：n</em>1行的矩阵，有n个元素，R^n<br>一般用大写字母表示矩阵  </p>
<h4 id="3-2-加法和标量乘法"><a href="#3-2-加法和标量乘法" class="headerlink" title="3.2 加法和标量乘法"></a>3.2 加法和标量乘法</h4><p>加法：</p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & 0 \\ 2 & 5 \\ 3 & 1\end{bmatrix}+\begin{bmatrix} 4 & 0.5 \\ 2 & 5 \\ 0 & 1\end{bmatrix}=\begin{bmatrix} 5 & 0.5 \\ 4 & 10 \\ 3 & 2\end{bmatrix}</script><p>这两个矩阵的每一个元素都逐个相加<br>标量乘法：<br>将标量与矩阵中每一个元素相乘 </p>
<h4 id="3-3-矩阵向量乘法"><a href="#3-3-矩阵向量乘法" class="headerlink" title="3.3 矩阵向量乘法"></a>3.3 矩阵向量乘法</h4><p>一个矩阵与向量相乘<br>m*n矩阵乘n*1向量 等于m*1向量<br>To get $y_i$,multiply A’s $i^{th}$ row with elements of vector x, and add them up.</p>
<h4 id="3-4-矩阵乘法"><a href="#3-4-矩阵乘法" class="headerlink" title="3.4 矩阵乘法"></a>3.4 矩阵乘法</h4><p>能够相乘的矩阵，需要满足矩阵的维度相互匹配<br><img src= "/img/loading.gif" data-src="/img/machine-learning/06.png" alt="图6">  </p>
<h4 id="3-5-矩阵乘法的特征"><a href="#3-5-矩阵乘法的特征" class="headerlink" title="3.5 矩阵乘法的特征"></a>3.5 矩阵乘法的特征</h4><p>标量的运算，乘法可以满足交换律，但是这个不能应用在矩阵运算中<br>矩阵运算满足结合律<br>特征矩阵：Identity Matrix</p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 &1\end{bmatrix}</script><p>对角线上都是1,其他位置都是0,对于任何矩阵A </p>
<script type="math/tex; mode=display">A\cdot I = I \cdot A = A</script><h4 id="3-6-逆和转置"><a href="#3-6-逆和转置" class="headerlink" title="3.6 逆和转置"></a>3.6 逆和转置</h4><p>如果A矩阵是一个m$\times$m的矩阵(square)，而且它存在一个逆矩阵，</p>
<script type="math/tex; mode=display">AA^{-1}=A^{-1}A=I</script><script type="math/tex; mode=display">\begin{bmatrix} 3 & 4 \\ 2 & 16 \end{bmatrix}\begin{bmatrix} 0.4 & -0.1 \\ -0.05 & 0.075 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I_{2\times 2}</script><p>不存在逆矩阵的矩阵称作奇异矩阵  其值无限接近0<br>转置矩阵：  </p>
<script type="math/tex; mode=display">A=\begin{bmatrix} 1 & 2 & 0 \\ 3 & 5 & 9\end{bmatrix}</script><script type="math/tex; mode=display">A^T=\begin{bmatrix} 1 & 3 \\ 2 & 5 \\ 0 & 9 \end{bmatrix}</script><h3 id="4-多变量线性回归"><a href="#4-多变量线性回归" class="headerlink" title="4 多变量线性回归"></a>4 多变量线性回归</h3><h4 id="4-1-多功能"><a href="#4-1-多功能" class="headerlink" title="4.1 多功能"></a>4.1 多功能</h4><p>n = number of features<br>$x^{(i)}$ = input(features) of i^{th} training example<br>$x^{(i)}_j$ =value of feature j in i^{th} training example  </p>
<p>假设函数：</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><p>为了方便，定义 $x_0=1$</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><h4 id="4-2-多元梯度下降法"><a href="#4-2-多元梯度下降法" class="headerlink" title="4.2 多元梯度下降法"></a>4.2 多元梯度下降法</h4><p>假设函数：$h_\theta(x)=\theta^Tx\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$<br>参数： $\theta_0,\theta_1,…,\theta_n$<br>代价函数：$J(\theta_0,\theta_1,…,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$<br>梯度下降：<br>Repeat  </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,...,\theta_n)</script><p>simultaneously update for every j = 0,…,n<br>New algorithm(n&gt;=1);<br>Repeat  </p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}</script><p>同时更新$\theta_j$, j=1,2,…,n<br>函数的特征需要保证相似的大小<br><strong>Mean normalizaton</strong><br>Replace $x_i$ with $x_i-\mu_i(平均值)$ to make features have approximately zero mean  (Do not apply to $x_0=1$)<br>例如$x_1$的范围是$0&lt;x_1\leq2000$, $x2$的范围是$0&lt;x_2\leq5$<br>特征缩放<br>$x_1=\frac{size-1000}{2000}$<br>$x_2=\frac{bedrooms-2}{5}$</p>
<script type="math/tex; mode=display">-0.5\leq x_1\leq0.5, -0.5 \leq x_2\leq0.5</script><p>怎么样选择学习率$\alpha$</p>
<h4 id="4-3-特征和多项目式回归"><a href="#4-3-特征和多项目式回归" class="headerlink" title="4.3 特征和多项目式回归"></a>4.3 特征和多项目式回归</h4><p>趋势可能是曲线，所以需要多项式，例如：</p>
<script type="math/tex; mode=display">\theta_0+\theta_1x+\theta_2x^2</script><p><img src= "/img/loading.gif" data-src="/img/machine-learning/07.png" alt="图7"></p>
<h3 id="5-正规方程"><a href="#5-正规方程" class="headerlink" title="5 正规方程"></a>5 正规方程</h3><p>对于某些线性回归问题，正规方程会给我们更好的方法来求得$\theta$的最优值。<br>使用解析的方式，跟递归不同，正规方程只需要一次就可求解<br>$\theta\in R^{n+1} \quad J(\theta_0,\theta_1,…,\theta_m)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y{(i)})^2$</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j}J(\theta)=...=0 \qquad (for\quad every\quad j)</script><p>solve for $\theta_0,\theta_1,..,\theta_n$<br><img src= "/img/loading.gif" data-src="/img/machine-learning/08.png" alt="图8"><br>增加一列$x_0$，它的取值永远是1<br>$X=\begin{bmatrix}1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45 \\ 1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40 \\ 1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30 \\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\end{bmatrix}\qquad y=\begin{bmatrix}460 \\232 \\315 \\178 \end{bmatrix}$  </p>
<script type="math/tex; mode=display">\theta=(X^TX)^{-1}X^Ty</script><p>使用正规方程不需要特征缩放，但递归下降算法适用于处理大量的数据，大于一万，小于一万用正规方程即可。<br>在矩阵方程不可逆的时候，使用程序可以正常计算出来（伪逆），这是技术性进阶的数学原因。而不可能逆有两种原因，数值的单位不一样，还有就是样本数太少，想求的参数太多，这样可以删除某些多余的特征。  </p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Guanghao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lguanghao.com/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/">http://lguanghao.com/2020/06/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lguanghao.com" target="_blank">Liu's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/machine-learning/">machine-learning</a></div><div class="post_share"><div class="social-share" data-image="/img/time.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/06/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"><img class="prev-cover" data-src="/img/ai.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">吴恩达机器学习（二）</div></div></a></div><div class="next-post pull-right"><a href="/2020/03/20/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89/"><img class="next-cover" data-src="/img/data.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据分析工具（三）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/07/22/Jupyter notebook快速上手/" title="Jupyter-notebook"><img class="relatedPosts_cover" data-src="/img/jupyter.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-22</div><div class="relatedPosts_title">Jupyter-notebook</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/02/吴恩达机器学习（三）/" title="吴恩达机器学习（三）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="relatedPosts_title">吴恩达机器学习（三）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/06/20/吴恩达机器学习（二）/" title="吴恩达机器学习（二）"><img class="relatedPosts_cover" data-src="/img/ai.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-20</div><div class="relatedPosts_title">吴恩达机器学习（二）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/06/Python时空数据分析1/" title="Python时空数据分析(一)"><img class="relatedPosts_cover" data-src="/img/time.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-06</div><div class="relatedPosts_title">Python时空数据分析(一)</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/26/列表的切片问题/" title="列表切片的问题"><img class="relatedPosts_cover" data-src="/img/white-dog.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-26</div><div class="relatedPosts_title">列表切片的问题</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/20/数据分析（三）/" title="数据分析工具（三）"><img class="relatedPosts_cover" data-src="/img/data.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-20</div><div class="relatedPosts_title">数据分析工具（三）</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(/img/mountains.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By Guanghao</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"right","hOffset":0,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body></html>